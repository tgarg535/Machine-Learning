{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### ✅**Theoretical Questions and Answers**\n",
        "\n",
        "---\n",
        "\n",
        "#### **1. What is Logistic Regression, and how does it differ from Linear Regression?**\n",
        "\n",
        "**Logistic Regression** is a classification algorithm used to predict discrete outcomes (e.g., binary labels 0 or 1).  \n",
        "**Linear Regression** predicts continuous outcomes.\n",
        "\n",
        "**Key Difference:**  \n",
        "- Logistic Regression uses the **sigmoid function** to output probabilities between 0 and 1.  \n",
        "- Linear Regression outputs real numbers without bounds.\n",
        "\n",
        "---\n",
        "\n",
        "#### **2. What is the mathematical equation of Logistic Regression?**\n",
        "\n",
        "\\[\n",
        "P(y=1|x) = \\frac{1}{1 + e^{-(w^Tx + b)}}\n",
        "\\]\n",
        "\n",
        "Where:  \n",
        "- \\( x \\): input features  \n",
        "- \\( w \\): weights  \n",
        "- \\( b \\): bias  \n",
        "- \\( P(y=1|x) \\): probability of class 1\n",
        "\n",
        "---\n",
        "\n",
        "#### **3. Why do we use the Sigmoid function in Logistic Regression?**\n",
        "\n",
        "The **sigmoid function** maps any real-valued number to a value between 0 and 1, which helps in interpreting the output as a **probability**.\n",
        "\n",
        "---\n",
        "\n",
        "#### **4. What is the cost function of Logistic Regression?**\n",
        "\n",
        "Logistic Regression uses **Log Loss (Binary Cross-Entropy)** as the cost function:\n",
        "\n",
        "\\[\n",
        "J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log(h(x^{(i)})) + (1 - y^{(i)}) \\log(1 - h(x^{(i)})) \\right]\n",
        "\\]\n",
        "\n",
        "---\n",
        "\n",
        "#### **5. What is Regularization in Logistic Regression? Why is it needed?**\n",
        "\n",
        "**Regularization** adds a penalty to the loss function to **prevent overfitting**.\n",
        "\n",
        "- It controls the complexity of the model by shrinking coefficients.\n",
        "- It helps in improving generalization on unseen data.\n",
        "\n",
        "---\n",
        "\n",
        "#### **6. Explain the difference between Lasso, Ridge, and Elastic Net Regression.**\n",
        "\n",
        "| Type        | Penalty Type     | Feature Selection |\n",
        "|-------------|------------------|-------------------|\n",
        "| Lasso       | L1               | Yes (shrinks to 0)|\n",
        "| Ridge       | L2               | No                |\n",
        "| Elastic Net | L1 + L2 combined | Yes               |\n",
        "\n",
        "---\n",
        "\n",
        "#### **7. When should we use Elastic Net instead of Lasso or Ridge?**\n",
        "\n",
        "Use **Elastic Net** when:\n",
        "- You suspect **multiple correlated features**.\n",
        "- You want the **feature selection of Lasso** + **stability of Ridge**.\n",
        "\n",
        "---\n",
        "\n",
        "#### **8. What is the impact of the regularization parameter (λ) in Logistic Regression?**\n",
        "\n",
        "- **λ (or C = 1/λ)** controls regularization strength.\n",
        "- A **higher λ** adds more penalty (simpler model).\n",
        "- A **lower λ** reduces penalty (risk of overfitting).\n",
        "\n",
        "---\n",
        "\n",
        "#### **9. What are the key assumptions of Logistic Regression?**\n",
        "\n",
        "- The response variable is **binary (or multiclass with extensions)**.\n",
        "- No multicollinearity between features.\n",
        "- Features are linearly related to the log-odds.\n",
        "- Large sample size for reliable predictions.\n",
        "\n",
        "---\n",
        "\n",
        "#### **10. What are some alternatives to Logistic Regression for classification tasks?**\n",
        "\n",
        "- Decision Trees  \n",
        "- Random Forest  \n",
        "- Support Vector Machines (SVM)  \n",
        "- K-Nearest Neighbors (KNN)  \n",
        "- Naive Bayes  \n",
        "- Neural Networks  \n",
        "\n",
        "---\n",
        "\n",
        "#### **11. What are Classification Evaluation Metrics?**\n",
        "\n",
        "- **Accuracy**  \n",
        "- **Precision**  \n",
        "- **Recall**  \n",
        "- **F1-Score**  \n",
        "- **ROC-AUC**  \n",
        "- **Confusion Matrix**  \n",
        "- **Cohen’s Kappa**  \n",
        "- **Matthews Correlation Coefficient (MCC)**  \n",
        "\n",
        "---\n",
        "\n",
        "#### **12. How does class imbalance affect Logistic Regression?**\n",
        "\n",
        "- It can cause the model to **favor the majority class**, leading to poor performance on the minority class.\n",
        "- Use techniques like:\n",
        "  - **class weights**\n",
        "  - **SMOTE**\n",
        "  - **resampling**\n",
        "\n",
        "---\n",
        "\n",
        "#### **13. What is Hyperparameter Tuning in Logistic Regression?**\n",
        "\n",
        "It involves choosing the best parameters like:\n",
        "- `C` (regularization strength)\n",
        "- `penalty` (l1, l2, elasticnet)\n",
        "- `solver` (optimization algorithm)\n",
        "\n",
        "Tools: `GridSearchCV`, `RandomizedSearchCV`\n",
        "\n",
        "---\n",
        "\n",
        "#### **14. What are different solvers in Logistic Regression? Which one should be used?**\n",
        "\n",
        "| Solver      | Best For                          |\n",
        "|-------------|-----------------------------------|\n",
        "| liblinear   | Binary, small datasets            |\n",
        "| saga        | Large datasets, elastic net       |\n",
        "| lbfgs       | Multiclass, fast for small data   |\n",
        "| newton-cg   | Multiclass                        |\n",
        "\n",
        "---\n",
        "\n",
        "#### **15. How is Logistic Regression extended for multiclass classification?**\n",
        "\n",
        "- **One-vs-Rest (OvR):** One classifier per class vs. all others.  \n",
        "- **Softmax (Multinomial):** Generalizes Logistic Regression to multiple classes.\n",
        "\n",
        "---\n",
        "\n",
        "#### **16. What are the advantages and disadvantages of Logistic Regression?**\n",
        "\n",
        "✅ Advantages:  \n",
        "- Simple and fast  \n",
        "- Easy to interpret  \n",
        "- Works well for linearly separable data\n",
        "\n",
        "❌ Disadvantages:  \n",
        "- Assumes linear decision boundary  \n",
        "- Poor performance with complex patterns  \n",
        "- Sensitive to outliers and imbalance\n",
        "\n",
        "---\n",
        "\n",
        "#### **17. What are some use cases of Logistic Regression?**\n",
        "\n",
        "- Email spam detection  \n",
        "- Fraud detection  \n",
        "- Customer churn prediction  \n",
        "- Disease diagnosis (e.g., cancer prediction)\n",
        "\n",
        "---\n",
        "\n",
        "#### **18. What is the difference between Softmax Regression and Logistic Regression?**\n",
        "\n",
        "- **Logistic Regression** handles **binary classification**.\n",
        "- **Softmax Regression** (a.k.a. Multinomial Logistic Regression) handles **multiclass classification** using softmax function.\n",
        "\n",
        "---\n",
        "\n",
        "#### **19. How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification?**\n",
        "\n",
        "- Use **OvR** for simplicity and interpretability.\n",
        "- Use **Softmax** for **mutually exclusive** classes and better performance on large multiclass problems.\n",
        "\n",
        "---\n",
        "\n",
        "#### **20. How do we interpret coefficients in Logistic Regression?**\n",
        "\n",
        "- Each coefficient shows the **change in log-odds** of the outcome with a one-unit increase in the predictor.\n",
        "- \\( \\text{Odds Ratio} = e^{\\text{coefficient}} \\)  \n",
        "  > >1 = increase in odds  \n",
        "  > <1 = decrease in odds\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ Practical Questions and Answers\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic Regression, and prints the model accuracy**\n",
        "\n",
        "```python\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Apply logistic regression\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and print accuracy\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy:\", accuracy)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1') and print the model accuracy**\n",
        "\n",
        "```python\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")  # to avoid convergence warnings\n",
        "\n",
        "# Load data\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# L1 Regularization\n",
        "model = LogisticRegression(penalty='l1', solver='liblinear', max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict & Accuracy\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"L1 Regularization Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Write a Python program to train Logistic Regression with L2 regularization (Ridge) using LogisticRegression(penalty='l2'). Print model accuracy and coefficients**\n",
        "\n",
        "```python\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# L2 Regularization (default)\n",
        "model = LogisticRegression(penalty='l2', solver='lbfgs', max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"L2 Regularization Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Model Coefficients:\", model.coef_)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet')**\n",
        "\n",
        "```python\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Elastic Net Regularization\n",
        "model = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5, max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict & Accuracy\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Elastic Net Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **5. Write a Python program to train a Logistic Regression model for multiclass classification using multi_class='ovr'**\n",
        "\n",
        "```python\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load multiclass dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# One-vs-Rest Logistic Regression\n",
        "model = LogisticRegression(multi_class='ovr', solver='lbfgs', max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"OvR Multiclass Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### **6. Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic Regression. Print the best parameters and accuracy**\n",
        "\n",
        "```python\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load data\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Hyperparameter grid\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10],\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear']\n",
        "}\n",
        "\n",
        "# Grid Search\n",
        "grid = GridSearchCV(LogisticRegression(max_iter=1000), param_grid, cv=5)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Best model\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Best Accuracy:\", grid.score(X_test, y_test))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **7. Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the average accuracy**\n",
        "\n",
        "```python\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Load data\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Stratified K-Fold\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Cross-validation\n",
        "scores = cross_val_score(model, X, y, cv=skf)\n",
        "print(\"Cross-Validation Accuracy Scores:\", scores)\n",
        "print(\"Average Accuracy:\", scores.mean())\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **8. Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its accuracy**\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset from CSV\n",
        "df = pd.read_csv('data.csv')  # Replace with your CSV file\n",
        "X = df.drop('target', axis=1)  # Replace 'target' with your label column\n",
        "y = df['target']\n",
        "\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Accuracy from CSV data:\", accuracy_score(y_test, y_pred))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **9. Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in Logistic Regression. Print the best parameters and accuracy**\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Load data\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Parameter grid\n",
        "param_dist = {\n",
        "    'C': np.logspace(-3, 3, 10),\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear', 'saga']\n",
        "}\n",
        "\n",
        "# Randomized Search\n",
        "rs = RandomizedSearchCV(LogisticRegression(max_iter=1000), param_distributions=param_dist, n_iter=10, cv=5)\n",
        "rs.fit(X_train, y_train)\n",
        "\n",
        "# Best results\n",
        "print(\"Best Parameters:\", rs.best_params_)\n",
        "print(\"Accuracy:\", rs.score(X_test, y_test))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **10. Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy**\n",
        "\n",
        "```python\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load data\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# OvO Logistic Regression\n",
        "ovo = OneVsOneClassifier(LogisticRegression(max_iter=1000))\n",
        "ovo.fit(X_train, y_train)\n",
        "\n",
        "# Accuracy\n",
        "y_pred = ovo.predict(X_test)\n",
        "print(\"One-vs-One Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "###  **11. Visualize the confusion matrix for binary classification**\n",
        "\n",
        "```python\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load data\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot()\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "###  **12. Evaluate model using Precision, Recall, and F1-Score**\n",
        "\n",
        "```python\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "print(\"Precision:\", precision_score(y_test, y_pred))\n",
        "print(\"Recall:\", recall_score(y_test, y_pred))\n",
        "print(\"F1 Score:\", f1_score(y_test, y_pred))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "###  **13. Train a Logistic Regression model on imbalanced data and apply class weights**\n",
        "\n",
        "```python\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Create imbalanced dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, weights=[0.9, 0.1], random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Class weights to handle imbalance\n",
        "model = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(\"Accuracy on imbalanced data:\", accuracy_score(y_test, y_pred))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "###  **14. Train Logistic Regression on the Titanic dataset, handle missing values, and evaluate performance**\n",
        "\n",
        "```python\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Titanic dataset\n",
        "titanic = sns.load_dataset(\"titanic\")\n",
        "\n",
        "# Select features and drop rows with missing target or key features\n",
        "titanic = titanic[[\"age\", \"sex\", \"fare\", \"survived\"]].dropna(subset=[\"fare\", \"sex\", \"survived\"])\n",
        "\n",
        "# Handle missing age values\n",
        "imputer = SimpleImputer(strategy=\"mean\")\n",
        "titanic[\"age\"] = imputer.fit_transform(titanic[[\"age\"]])\n",
        "\n",
        "# Encode categorical column\n",
        "titanic[\"sex\"] = LabelEncoder().fit_transform(titanic[\"sex\"])\n",
        "\n",
        "# Split features and label\n",
        "X = titanic.drop(\"survived\", axis=1)\n",
        "y = titanic[\"survived\"]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train and evaluate model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Titanic Dataset Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "###  **15. Apply feature scaling (Standardization) before training a Logistic Regression model and compare results**\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Without Scaling\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "acc_no_scaling = accuracy_score(y_test, model.predict(X_test))\n",
        "\n",
        "# With Scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "model_scaled = LogisticRegression(max_iter=1000)\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "acc_scaled = accuracy_score(y_test, model_scaled.predict(X_test_scaled))\n",
        "\n",
        "print(\"Accuracy without scaling:\", acc_no_scaling)\n",
        "print(\"Accuracy with scaling:\", acc_scaled)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "###  **16. Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC score**\n",
        "\n",
        "```python\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Load data\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities\n",
        "y_prob = model.predict_proba(X_test)[:, 1]\n",
        "auc = roc_auc_score(y_test, y_prob)\n",
        "print(\"ROC-AUC Score:\", auc)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "###  **17. Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate accuracy**\n",
        "\n",
        "```python\n",
        "model = LogisticRegression(C=0.5, max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Accuracy with C=0.5:\", accuracy_score(y_test, y_pred))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "###  **18. Write a Python program to train Logistic Regression and identify important features based on model coefficients**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Coefficients and feature importance\n",
        "importance = model.coef_[0]\n",
        "for i, v in enumerate(importance):\n",
        "    print(f\"Feature {i}: Coefficient = {v:.4f}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "###  **19. Write a Python program to train Logistic Regression and evaluate its performance using Cohen’s Kappa Score**\n",
        "\n",
        "```python\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "kappa = cohen_kappa_score(y_test, y_pred)\n",
        "print(\"Cohen’s Kappa Score:\", kappa)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "###  **20. Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary classification**\n",
        "\n",
        "```python\n",
        "from sklearn.metrics import precision_recall_curve, PrecisionRecallDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "y_scores = model.predict_proba(X_test)[:, 1]\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_scores)\n",
        "disp = PrecisionRecallDisplay(precision=precision, recall=recall)\n",
        "disp.plot()\n",
        "plt.title(\"Precision-Recall Curve\")\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "###  **21. Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare their accuracy**\n",
        "\n",
        "```python\n",
        "solvers = ['liblinear', 'saga', 'lbfgs']\n",
        "for solver in solvers:\n",
        "    model = LogisticRegression(solver=solver, max_iter=1000)\n",
        "    model.fit(X_train, y_train)\n",
        "    acc = accuracy_score(y_test, model.predict(X_test))\n",
        "    print(f\"Solver: {solver}, Accuracy: {acc}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "###  **22. Write a Python program to train Logistic Regression and evaluate its performance using Matthews Correlation Coefficient (MCC)**\n",
        "\n",
        "```python\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "mcc = matthews_corrcoef(y_test, y_pred)\n",
        "print(\"Matthews Correlation Coefficient (MCC):\", mcc)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "###  **23. Write a Python program to train Logistic Regression on both raw and standardized data. Compare their accuracy to see the impact of feature scaling**\n",
        "\n",
        "```python\n",
        "# Raw data\n",
        "model_raw = LogisticRegression(max_iter=1000)\n",
        "model_raw.fit(X_train, y_train)\n",
        "acc_raw = accuracy_score(y_test, model_raw.predict(X_test))\n",
        "\n",
        "# Scaled data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "model_scaled = LogisticRegression(max_iter=1000)\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "acc_scaled = accuracy_score(y_test, model_scaled.predict(X_test_scaled))\n",
        "\n",
        "print(\"Accuracy (Raw):\", acc_raw)\n",
        "print(\"Accuracy (Standardized):\", acc_scaled)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "###  **24. Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using cross-validation**\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import numpy as np\n",
        "\n",
        "C_values = [0.01, 0.1, 1, 10]\n",
        "for C in C_values:\n",
        "    model = LogisticRegression(C=C, max_iter=1000)\n",
        "    scores = cross_val_score(model, X, y, cv=5)\n",
        "    print(f\"C={C}, Cross-Validation Accuracy: {np.mean(scores):.4f}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "###  **25. Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to make predictions**\n",
        "\n",
        "```python\n",
        "import joblib\n",
        "\n",
        "# Train model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Save model\n",
        "joblib.dump(model, \"logistic_model.pkl\")\n",
        "\n",
        "# Load and predict\n",
        "loaded_model = joblib.load(\"logistic_model.pkl\")\n",
        "y_pred = loaded_model.predict(X_test)\n",
        "print(\"Accuracy of loaded model:\", accuracy_score(y_test, y_pred))\n",
        "```\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "CSeLyeLCnq66"
      }
    }
  ]
}