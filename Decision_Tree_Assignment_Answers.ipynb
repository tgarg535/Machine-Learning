{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tgarg535/Machine-Learning/blob/main/Decision_Tree_Assignment_Answers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "## Decision Tree Assignment\n",
        "---\n",
        "\n",
        "### Question 1: What is a Decision Tree, and how does it work in the context of classification?\n",
        "\n",
        "**Answer:**\n",
        "A **Decision Tree** is a supervised learning algorithm used for both classification and regression tasks. It has a hierarchical, tree-like structure consisting of a root node, branches, internal nodes, and leaf nodes.\n",
        "\n",
        "In the context of **classification**, it works by:\n",
        "\n",
        "1. **Recursive Partitioning:** Starting at the root, the algorithm selects the \"best\" feature to split the data into subsets that are as homogeneous (pure) as possible.\n",
        "2. **Decision Rules:** Each internal node represents a \"test\" on an attribute (e.g., \"Is age > 30?\"), each branch represents the outcome of the test, and each leaf node represents a class label.\n",
        "3. **Path to Prediction:** To classify a new data point, you follow the path from the root down to a leaf node based on the feature values of that point. The class assigned to that leaf becomes the prediction.\n",
        "\n",
        "---\n",
        "\n",
        "### Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?\n",
        "\n",
        "**Answer:**\n",
        "Both Gini Impurity and Entropy are mathematical metrics used to evaluate the quality of a split.\n",
        "\n",
        "* **Gini Impurity:** Measures the frequency at which a randomly chosen element from the set would be incorrectly labeled if it was randomly labeled according to the distribution of labels in the subset. It is calculated as:\n",
        "\n",
        "\n",
        "* **Entropy:** Originating from information theory, it measures the level of disorder or uncertainty in the data. It is calculated as:\n",
        "\n",
        "\n",
        "\n",
        "**Impact on Splits:**\n",
        "The algorithm calculates these values for every possible split. It chooses the split that results in the **lowest** impurity (or highest \"purity gain\"). Gini is computationally faster because it doesn't involve logarithmic calculations, while Entropy can sometimes produce slightly more balanced trees.\n",
        "\n",
        "---\n",
        "\n",
        "### Question 3: What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.\n",
        "\n",
        "**Answer:**\n",
        "Pruning is the process of reducing the size of a tree to prevent **overfitting**.\n",
        "\n",
        "| Feature | Pre-Pruning (Early Stopping) | Post-Pruning (Cost Complexity Pruning) |\n",
        "| --- | --- | --- |\n",
        "| **Method** | Stops the tree from growing once it hits a certain threshold (e.g., max depth). | Grows the full tree first, then removes branches that provide little predictive power. |\n",
        "| **Advantage** | **Efficiency:** Saves time and computation by not building a massive, unnecessary tree. | **Precision:** Often more accurate because it sees the whole data structure before deciding what is \"noise.\" |\n",
        "\n",
        "---\n",
        "\n",
        "### Question 4: What is Information Gain in Decision Trees, and why is it important for choosing the best split?\n",
        "\n",
        "**Answer:**\n",
        "**Information Gain** is the decrease in Entropy after a dataset is split on an attribute. It is the primary criterion used by algorithms like ID3.\n",
        "\n",
        "**Importance:**\n",
        "It acts as a selection tool. By calculating Information Gain for every feature, the algorithm can mathematically determine which feature \"organizes\" the data most effectively. The feature with the highest Information Gain is chosen as the splitting node for that level.\n",
        "\n",
        "---\n",
        "\n",
        "### Question 5: What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "* **Real-world Applications:** Credit scoring (loan approval), medical diagnosis, customer churn prediction, and fraud detection.\n",
        "* **Advantages:**\n",
        "* **Interpretability:** They are \"white-box\" models; humans can easily understand the logic.\n",
        "* **Data Prep:** Requires very little data scaling or normalization.\n",
        "* **Versatility:** Handles both numerical and categorical data.\n",
        "\n",
        "\n",
        "* **Limitations:**\n",
        "* **Instability:** Small changes in data can result in a completely different tree.\n",
        "* **Overfitting:** High tendency to create over-complex trees that don't generalize well.\n",
        "* **Bias:** Can be biased toward features with more levels/categories.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### Question 6: Python Program - Basic Iris Classification\n",
        "\n",
        "**Answer:**"
      ],
      "metadata": {
        "id": "1lcGF8Y8CkeS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split and Train\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Output\n",
        "y_pred = clf.predict(X_test)\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n",
        "print(\"Feature Importances:\", dict(zip(iris.feature_names, clf.feature_importances_)))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "LcPumGqzCkeV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Output:**\n",
        "\n",
        "* **Accuracy:** 1.00\n",
        "* **Feature Importances:** {'sepal length (cm)': 0.0, 'sepal width (cm)': 0.0, 'petal length (cm)': 0.906, 'petal width (cm)': 0.093}\n",
        "\n",
        "---\n",
        "\n",
        "### Question 7: Python Program - Max Depth Comparison\n",
        "\n",
        "**Answer:**"
      ],
      "metadata": {
        "id": "3V0YQwVVCkeX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fully grown tree\n",
        "clf_full = DecisionTreeClassifier(random_state=42).fit(X_train, y_train)\n",
        "# Restricted tree\n",
        "clf_depth3 = DecisionTreeClassifier(max_depth=3, random_state=42).fit(X_train, y_train)\n",
        "\n",
        "print(f\"Fully Grown Accuracy: {clf_full.score(X_test, y_test):.2f}\")\n",
        "print(f\"Max Depth 3 Accuracy: {clf_depth3.score(X_test, y_test):.2f}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "KJTZlREECkeX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Output:**\n",
        "\n",
        "* **Fully Grown Accuracy:** 1.00\n",
        "* **Max Depth 3 Accuracy:** 1.00\n",
        "*(Note: On the simple Iris set, a depth of 3 is often sufficient to reach max accuracy).*\n",
        "\n",
        "---\n",
        "\n",
        "### Question 8: Python Program - Regression on California Housing\n",
        "\n",
        "**Answer:**"
      ],
      "metadata": {
        "id": "Ds15qVAQCkeY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load Dataset\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target\n",
        "\n",
        "# Split and Train\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "reg = DecisionTreeRegressor(random_state=42)\n",
        "reg.fit(X_train, y_train)\n",
        "\n",
        "# Output\n",
        "y_pred = reg.predict(X_test)\n",
        "print(f\"MSE: {mean_squared_error(y_test, y_pred):.4f}\")\n",
        "print(\"Top Feature Importance (MedInc):\", reg.feature_importances_[0])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "RAdfopC5CkeY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Output:**\n",
        "\n",
        "* **MSE:** 0.4914\n",
        "* **Top Feature Importance (MedInc):** 0.525...\n",
        "\n",
        "---\n",
        "\n",
        "### Question 9: Python Program - Hyperparameter Tuning\n",
        "\n",
        "**Answer:**"
      ],
      "metadata": {
        "id": "y69652TwCkeZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'max_depth': [3, 5, 10, None],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(f\"Best Accuracy: {grid_search.best_score_:.2f}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "FddxnrsSCkeZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Output:**\n",
        "\n",
        "* **Best Parameters:** {'max_depth': 3, 'min_samples_split': 2}\n",
        "* **Best Accuracy:** 0.96\n",
        "\n",
        "---\n",
        "\n",
        "### Question 10: Healthcare Case Study\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Step-by-Step Process:**\n",
        "\n",
        "1. **Handle Missing Values:** For numerical data, use median imputation (robust to outliers). For categorical data, use mode imputation or create a \"Missing\" category.\n",
        "2. **Encode Categorical Features:** Use **One-Hot Encoding** for nominal data (e.g., Blood Type) or **Label Encoding** for ordinal data (e.g., Disease Stage: Mild, Moderate, Severe).\n",
        "3. **Train Decision Tree:** Split data into 80/20 train/test sets and fit a `DecisionTreeClassifier`.\n",
        "4. **Tune Hyperparameters:** Use `GridSearchCV` to find the optimal `max_depth` and `min_samples_leaf` to ensure the model isn't just \"memorizing\" the training patients (overfitting).\n",
        "5. **Evaluate Performance:** Focus on **Recall** (Sensitivity) and **F1-Score**. In healthcare, missing a sick patient (False Negative) is usually much worse than a false alarm.\n",
        "\n",
        "**Business Value:**\n",
        "This model provides value through **Early Intervention**. By identifying high-risk patients automatically, the company can allocate medical resources more efficiently, reduce hospitalization costs through preventative care, and ultimately save lives by catching diseases before they become critical.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "65tUilHiCkea"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}