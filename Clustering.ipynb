{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tgarg535/Machine-Learning/blob/main/Clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Theoretical Questions:**\n",
        "### **1. What is unsupervised learning in the context of machine learning?**\n",
        "\n",
        "Unsupervised learning is a type of machine learning where the algorithm learns from data that has **not been labeled** or categorized. Unlike supervised learning, there is no \"ground truth\" or target variable. The goal is to find hidden patterns, structures, or groupings within the data.\n",
        "\n",
        "### **2. How does the K-Means clustering algorithm work?**\n",
        "\n",
        "K-Means follows an iterative process to partition data into  distinct clusters:\n",
        "\n",
        "1. **Initialization:** Select  random points as initial centroids.\n",
        "2. **Assignment:** Assign each data point to the nearest centroid based on a distance metric (usually Euclidean distance).\n",
        "3. **Update:** Calculate the mean of all points in each cluster and move the centroid to that mean position.\n",
        "4. **Repeat:** Steps 2 and 3 are repeated until centroids no longer change or a maximum number of iterations is reached.\n",
        "\n",
        "### **3. Explain the concept of a dendrogram in hierarchical clustering.**\n",
        "\n",
        "A dendrogram is a **tree-like diagram** that records the sequences of merges or splits during hierarchical clustering.\n",
        "\n",
        "* The **y-axis** represents the distance (dissimilarity) between clusters.\n",
        "* The **x-axis** represents individual data points.\n",
        "* By cutting the dendrogram horizontally at a certain height, you can determine the number of clusters in the dataset.\n",
        "\n",
        "### **4. What is the main difference between K-Means and Hierarchical Clustering?**\n",
        "\n",
        "* **K-Means:** Requires the number of clusters () to be specified in advance. It is computationally efficient for large datasets and produces \"flat\" partitions.\n",
        "* **Hierarchical Clustering:** Does not require a pre-defined number of clusters. It creates a nested structure (hierarchy) of clusters, which is better for understanding data relationships but is computationally expensive ( or ).\n",
        "\n",
        "### **5. What are the advantages of DBSCAN over K-Means?**\n",
        "\n",
        "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) offers three main advantages:\n",
        "\n",
        "1. **Arbitrary Shapes:** It can find clusters of any shape, whereas K-Means assumes clusters are spherical.\n",
        "2. **No K-Value:** You don’t need to specify the number of clusters beforehand.\n",
        "3. **Noise Handling:** It explicitly identifies and ignores outliers (noise points).\n",
        "\n",
        "### **6. When would you use Silhouette Score in clustering?**\n",
        "\n",
        "The Silhouette Score is used to evaluate the **quality and separation** of clusters. It measures how similar an object is to its own cluster compared to other clusters. You use it when you want to validate the choice of  or compare different clustering algorithms.\n",
        "\n",
        "### **7. What are the limitations of Hierarchical Clustering?**\n",
        "\n",
        "* **Scalability:** It is very slow on large datasets because the complexity increases drastically with the number of points.\n",
        "* **Irreversibility:** Once a merge or split is done, it cannot be undone.\n",
        "* **Sensitivity:** It is sensitive to noise and outliers.\n",
        "\n",
        "### **8. Why is feature scaling important in clustering algorithms like K-Means?**\n",
        "\n",
        "K-Means relies on **distance calculations** (like Euclidean distance). If one feature has a range of 0–1 and another has a range of 0–10,000, the larger feature will dominate the distance calculation, making the smaller feature irrelevant. Scaling ensures all features contribute equally.\n",
        "\n",
        "### **9. How does DBSCAN identify noise points?**\n",
        "\n",
        "DBSCAN classifies a point as **Noise** if it is neither a \"Core Point\" (having at least `minPts` within a radius `eps`) nor a \"Border Point\" (within the radius of a Core Point but having fewer than `minPts` neighbors).\n",
        "\n",
        "### **10. Define inertia in the context of K-Means.**\n",
        "\n",
        "Inertia (or Within-Cluster Sum of Squares) is the sum of squared distances of samples to their closest cluster center. It measures how **internally coherent** clusters are; lower inertia indicates more tightly packed clusters.\n",
        "\n",
        "---\n",
        "\n",
        "### **11. What is the elbow method in K-Means clustering?**\n",
        "\n",
        "The elbow method is a heuristic used to find the optimal number of clusters (). You plot the **Inertia** against the number of clusters. The \"elbow\" point on the graph—where the rate of decrease in inertia slows down significantly—is usually considered the ideal .\n",
        "\n",
        "### **12. Describe the concept of \"density\" in DBSCAN.**\n",
        "\n",
        "Density is defined by the number of points within a specific neighborhood. A region is considered \"dense\" if it contains at least a minimum number of points (`minPts`) within a specified radius (`eps`). Clusters are essentially high-density regions separated by low-density regions.\n",
        "\n",
        "### **13. Can hierarchical clustering be used on categorical data?**\n",
        "\n",
        "Yes, but you cannot use Euclidean distance. You must use appropriate dissimilarity measures for categorical data, such as **Gower’s distance** or **Jaccard similarity**, before applying the clustering algorithm.\n",
        "\n",
        "### **14. What does a negative Silhouette Score indicate?**\n",
        "\n",
        "A negative value (closer to -1) indicates that a sample has been assigned to the **wrong cluster**, as it is more similar to a neighboring cluster than the one it currently belongs to.\n",
        "\n",
        "### **15. Explain the term \"linkage criteria\" in hierarchical clustering.**\n",
        "\n",
        "Linkage criteria determine how the distance between two *clusters* is calculated:\n",
        "\n",
        "* **Single Linkage:** Distance between the closest points of two clusters.\n",
        "* **Complete Linkage:** Distance between the farthest points.\n",
        "* **Average Linkage:** Average distance between all pairs of points.\n",
        "* **Ward’s Linkage:** Minimizes the variance within clusters.\n",
        "\n",
        "### **16. Why might K-Means clustering perform poorly on data with varying cluster sizes or densities?**\n",
        "\n",
        "K-Means tries to minimize the variance, which leads it to prefer clusters of **similar size and spherical shape**. If one cluster is very dense and another is very sparse, or if one is much larger than the other, K-Means may \"break\" the large/sparse cluster to balance the inertia.\n",
        "\n",
        "### **17. What are the core parameters in DBSCAN, and how do they influence clustering?**\n",
        "\n",
        "1. **Epsilon (eps):** The maximum distance between two samples for one to be considered as in the neighborhood of the other. If `eps` is too small, most data will be noise; if too large, clusters will merge.\n",
        "2. **MinPts:** The number of samples in a neighborhood for a point to be considered a core point. Higher `MinPts` works better for noisy datasets.\n",
        "\n",
        "### **18. How does K-Means++ improve upon standard K-Means initialization?**\n",
        "\n",
        "Standard K-Means picks initial centroids randomly, which can lead to poor convergence. **K-Means++** spreads out the initial centroids by choosing the first one randomly and then picking subsequent centroids with a probability proportional to their squared distance from the closest existing centroid.\n",
        "\n",
        "### **19. What is agglomerative clustering?**\n",
        "\n",
        "Agglomerative clustering is the most common type of hierarchical clustering. It is a **\"bottom-up\"** approach where each observation starts in its own cluster, and pairs of clusters are merged as one moves up the hierarchy.\n",
        "\n",
        "### **20. What makes Silhouette Score a better metric than just inertia for model evaluation?**\n",
        "\n",
        "Inertia only measures how tight clusters are (cohesion), but it always decreases as you add more clusters (even if it leads to overfitting). **Silhouette Score** considers both **cohesion** (how close points are to their own center) and **separation** (how far points are from other clusters), providing a more balanced view of cluster quality.\n",
        "\n",
        "---\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "FdGJMEldwcoC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Practical Questions**\n",
        "\n",
        "### **1. K-Means with 4 Centers (make_blobs)**\n",
        "\n",
        "```python\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X, y = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)\n",
        "kmeans = KMeans(n_clusters=4)\n",
        "kmeans.fit(X)\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=kmeans.labels_, cmap='viridis')\n",
        "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='red', marker='X')\n",
        "plt.title(\"K-Means with 4 Clusters\")\n",
        "plt.show()\n",
        "\n",
        "```\n",
        "\n",
        "### **2. Agglomerative Clustering on Iris Dataset**\n",
        "\n",
        "```python\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "iris = load_iris()\n",
        "agg_clustering = AgglomerativeClustering(n_clusters=3)\n",
        "labels = agg_clustering.fit_predict(iris.data)\n",
        "\n",
        "print(\"First 10 predicted labels:\", labels[:10])\n",
        "\n",
        "```\n",
        "\n",
        "### **3. DBSCAN on Moon Data (Highlighting Outliers)**\n",
        "\n",
        "```python\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "X, _ = make_moons(n_samples=200, noise=0.05, random_state=0)\n",
        "dbscan = DBSCAN(eps=0.2, min_samples=5)\n",
        "labels = dbscan.fit_predict(X)\n",
        "\n",
        "plt.scatter(X[labels != -1, 0], X[labels != -1, 1], c=labels[labels != -1], cmap='Paired')\n",
        "plt.scatter(X[labels == -1, 0], X[labels == -1, 1], c='black', label='Outliers')\n",
        "plt.legend()\n",
        "plt.title(\"DBSCAN: Clusters and Noise\")\n",
        "plt.show()\n",
        "\n",
        "```\n",
        "\n",
        "### **4. Wine Dataset: Standardization & K-Means**\n",
        "\n",
        "```python\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "\n",
        "wine = load_wine()\n",
        "X_scaled = StandardScaler().fit_transform(wine.data)\n",
        "kmeans = KMeans(n_clusters=3, random_state=42).fit(X_scaled)\n",
        "\n",
        "clusters = pd.Series(kmeans.labels_)\n",
        "print(\"Cluster Sizes:\\n\", clusters.value_counts())\n",
        "\n",
        "```\n",
        "\n",
        "### **5. DBSCAN on Concentric Circles**\n",
        "\n",
        "```python\n",
        "from sklearn.datasets import make_circles\n",
        "\n",
        "X, _ = make_circles(n_samples=500, factor=0.5, noise=0.05)\n",
        "dbscan = DBSCAN(eps=0.15, min_samples=5)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=dbscan.fit_predict(X), cmap='plasma')\n",
        "plt.title(\"DBSCAN on Concentric Circles\")\n",
        "plt.show()\n",
        "\n",
        "```\n",
        "\n",
        "### **6. Breast Cancer Dataset: MinMaxScaler & Centroids**\n",
        "\n",
        "```python\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X_scaled = MinMaxScaler().fit_transform(data.data)\n",
        "kmeans = KMeans(n_clusters=2, random_state=42).fit(X_scaled)\n",
        "\n",
        "print(\"Cluster Centroids:\\n\", kmeans.cluster_centers_)\n",
        "\n",
        "```\n",
        "\n",
        "### **7. DBSCAN with Varying Cluster Densities**\n",
        "\n",
        "```python\n",
        "X, _ = make_blobs(n_samples=500, centers=3, cluster_std=[0.5, 1.5, 0.7], random_state=42)\n",
        "dbscan = DBSCAN(eps=0.8, min_samples=5)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=dbscan.fit_predict(X), cmap='tab10')\n",
        "plt.title(\"DBSCAN with Varying Densities\")\n",
        "plt.show()\n",
        "\n",
        "```\n",
        "\n",
        "### **8. PCA Reduction & K-Means (Digits Dataset)**\n",
        "\n",
        "```python\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "digits = load_digits()\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(digits.data)\n",
        "\n",
        "kmeans = KMeans(n_clusters=10, random_state=42).fit(X_pca)\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=kmeans.labels_, cmap='Spectral', s=5)\n",
        "plt.title(\"K-Means on Digits (PCA Reduced)\")\n",
        "plt.show()\n",
        "\n",
        "```\n",
        "\n",
        "### **9. Silhouette Scores Bar Chart (k=2 to 5)**\n",
        "\n",
        "```python\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "X, _ = make_blobs(n_samples=500, centers=4, random_state=42)\n",
        "scores = []\n",
        "ks = [2, 3, 4, 5]\n",
        "\n",
        "for k in ks:\n",
        "    labels = KMeans(n_clusters=k, random_state=42).fit_predict(X)\n",
        "    scores.append(silhouette_score(X, labels))\n",
        "\n",
        "plt.bar(ks, scores, color='skyblue')\n",
        "plt.xlabel('Number of Clusters (k)')\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.show()\n",
        "\n",
        "```\n",
        "\n",
        "### **10. Dendrogram for Iris (Average Linkage)**\n",
        "\n",
        "```python\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "\n",
        "iris = load_iris()\n",
        "linked = linkage(iris.data, method='average')\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "dendrogram(linked)\n",
        "plt.title(\"Iris Dendrogram (Average Linkage)\")\n",
        "plt.show()\n",
        "\n",
        "```\n",
        "\n",
        "### **11. K-Means with Decision Boundaries**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "X, _ = make_blobs(n_samples=300, centers=3, cluster_std=2.0, random_state=42)\n",
        "kmeans = KMeans(n_clusters=3).fit(X)\n",
        "\n",
        "h = .02 # Mesh step size\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
        "\n",
        "Z = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "plt.contourf(xx, yy, Z, alpha=0.3)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=kmeans.labels_, edgecolor='k')\n",
        "plt.title(\"K-Means Decision Boundaries\")\n",
        "plt.show()\n",
        "\n",
        "```\n",
        "\n",
        "### **12. t-SNE + DBSCAN (Digits Dataset)**\n",
        "\n",
        "```python\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "digits = load_digits()\n",
        "X_tsne = TSNE(n_components=2, random_state=42).fit_transform(digits.data)\n",
        "dbscan = DBSCAN(eps=3, min_samples=10).fit(X_tsne)\n",
        "\n",
        "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=dbscan.labels_, cmap='tab10', s=5)\n",
        "plt.title(\"t-SNE + DBSCAN (Digits)\")\n",
        "plt.show()\n",
        "\n",
        "```\n",
        "\n",
        "### **13. Agglomerative Clustering (Complete Linkage)**\n",
        "\n",
        "```python\n",
        "X, _ = make_blobs(n_samples=300, centers=4, random_state=42)\n",
        "agg = AgglomerativeClustering(n_clusters=4, linkage='complete')\n",
        "plt.scatter(X[:, 0], X[:, 1], c=agg.fit_predict(X), cmap='viridis')\n",
        "plt.show()\n",
        "\n",
        "```\n",
        "\n",
        "### **14. Elbow Method: Inertia for K=2 to 6**\n",
        "\n",
        "```python\n",
        "cancer = load_breast_cancer()\n",
        "inertia = []\n",
        "for k in range(2, 7):\n",
        "    km = KMeans(n_clusters=k, random_state=42).fit(cancer.data)\n",
        "    inertia.append(km.inertia_)\n",
        "\n",
        "\n",
        "plt.plot(range(2, 7), inertia, marker='o')\n",
        "plt.xlabel('K')\n",
        "plt.ylabel('Inertia')\n",
        "plt.show()\n",
        "\n",
        "```\n",
        "\n",
        "### **15. Agglomerative with Single Linkage (Circles)**\n",
        "\n",
        "```python\n",
        "X, _ = make_circles(n_samples=500, factor=0.5, noise=0.05)\n",
        "agg = AgglomerativeClustering(n_clusters=2, linkage='single')\n",
        "plt.scatter(X[:, 0], X[:, 1], c=agg.fit_predict(X))\n",
        "plt.title(\"Agglomerative (Single Linkage) on Circles\")\n",
        "plt.show()\n",
        "\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **16–20: Quick Highlights**\n",
        "\n",
        "* **Wine + DBSCAN:** Always use `StandardScaler` first. To count clusters: `len(set(labels)) - (1 if -1 in labels else 0)`.\n",
        "* **Iris Noise:** DBSCAN with `eps=0.5` on raw Iris data usually results in several noise points (`-1`).\n",
        "* **Non-Linear Moons:** K-Means will fail (splitting the moons vertically or horizontally), while DBSCAN will succeed.\n",
        "* **3D Digits:** Use `PCA(n_components=3)` and `ax = fig.add_subplot(111, projection='3d')`.\n",
        "\n",
        "---\n",
        "\n",
        "### **21. Java + DSA (Quick Summary)**\n",
        "\n",
        "If you are implementing these in Java, you would typically use libraries like **Weka** or **Apache Commons Math**. For DSA:\n",
        "\n",
        "* **K-Means Complexity:**  (n=points, k=clusters, i=iterations, d=dimensions).\n",
        "* **Hierarchical Complexity:**  for basic implementations, can be optimized to .\n",
        "\n",
        "---\n",
        "\n",
        "### **22. Silhouette Score Evaluation (5 centers)**\n",
        "\n",
        "```python\n",
        "X, _ = make_blobs(n_samples=500, centers=5, random_state=42)\n",
        "km = KMeans(n_clusters=5).fit(X)\n",
        "print(f\"Silhouette Score for K=5: {silhouette_score(X, km.labels_):.3f}\")\n",
        "\n",
        "```\n",
        "\n",
        "### **23. PCA + Agglomerative (Breast Cancer)**\n",
        "\n",
        "```python\n",
        "X_pca = PCA(n_components=2).fit_transform(StandardScaler().fit_transform(cancer.data))\n",
        "agg = AgglomerativeClustering(n_clusters=2).fit_predict(X_pca)\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=agg)\n",
        "plt.show()\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "OP1AOFOIw-Ac"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **24. K-Means vs. DBSCAN Side-by-Side (Noisy Circles)**\n",
        "\n",
        "This comparison highlights how K-Means fails at non-linear geometry while DBSCAN excels.\n",
        "\n",
        "```python\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_circles\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Generate data\n",
        "X, _ = make_circles(n_samples=500, factor=0.5, noise=0.08, random_state=42)\n",
        "X = StandardScaler().fit_transform(X)\n",
        "\n",
        "# Apply Algorithms\n",
        "km = KMeans(n_clusters=2, random_state=42).fit_predict(X)\n",
        "db = DBSCAN(eps=0.3, min_samples=5).fit_predict(X)\n",
        "\n",
        "# Visualization\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "ax1.scatter(X[:, 0], X[:, 1], c=km, cmap='viridis')\n",
        "ax1.set_title(\"K-Means (Failed Structure)\")\n",
        "ax2.scatter(X[:, 0], X[:, 1], c=db, cmap='plasma')\n",
        "ax2.set_title(\"DBSCAN (Successful Structure)\")\n",
        "plt.show()\n",
        "\n",
        "```\n",
        "\n",
        "### **25. Silhouette Coefficient Plot per Sample (Iris)**\n",
        "\n",
        "Unlike a single score, this plot shows how well each individual sample fits its cluster.\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import matplotlib.cm as cm\n",
        "from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "n_clusters = 3\n",
        "km = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "labels = km.fit_predict(X)\n",
        "\n",
        "score = silhouette_score(X, labels)\n",
        "sample_values = silhouette_samples(X, labels)\n",
        "\n",
        "y_lower = 10\n",
        "for i in range(n_clusters):\n",
        "    ith_values = sample_values[labels == i]\n",
        "    ith_values.sort()\n",
        "    size_cluster_i = ith_values.shape[0]\n",
        "    y_upper = y_lower + size_cluster_i\n",
        "    color = cm.nipy_spectral(float(i) / n_clusters)\n",
        "    plt.fill_betweenx(np.arange(y_lower, y_upper), 0, ith_values, facecolor=color)\n",
        "    y_lower = y_upper + 10\n",
        "\n",
        "\n",
        "plt.title(\"Silhouette Plot for Iris Clusters\")\n",
        "plt.xlabel(\"Silhouette Coefficient\")\n",
        "plt.show()\n",
        "\n",
        "```\n",
        "\n",
        "### **26. Agglomerative Clustering with 'Average' Linkage**\n",
        "\n",
        "Average linkage is often more robust than single linkage as it considers all pairs between clusters.\n",
        "\n",
        "```python\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "X, _ = make_blobs(n_samples=300, centers=3, random_state=42)\n",
        "agg = AgglomerativeClustering(n_clusters=3, linkage='average')\n",
        "labels = agg.fit_predict(X)\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='rainbow')\n",
        "plt.title(\"Agglomerative Clustering: Average Linkage\")\n",
        "plt.show()\n",
        "\n",
        "```\n",
        "\n",
        "### **27. Wine Dataset: Seaborn Pairplot of Assignments**\n",
        "\n",
        "This is a great way to visualize how clusters separate across multiple dimensions.\n",
        "\n",
        "```python\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_wine\n",
        "\n",
        "wine = load_wine()\n",
        "X_scaled = StandardScaler().fit_transform(wine.data)\n",
        "km = KMeans(n_clusters=3, random_state=42).fit(X_scaled)\n",
        "\n",
        "# Create DataFrame with first 4 features and labels\n",
        "df = pd.DataFrame(wine.data[:, :4], columns=wine.feature_names[:4])\n",
        "df['cluster'] = km.labels_\n",
        "\n",
        "sns.pairplot(df, hue='cluster', palette='bright')\n",
        "plt.show()\n",
        "\n",
        "```\n",
        "\n",
        "### **28. Noisy Blobs: DBSCAN Identification and Counts**\n",
        "\n",
        "DBSCAN categorizes noise as `-1`. Here is how to programmatically count them.\n",
        "\n",
        "```python\n",
        "X, _ = make_blobs(n_samples=400, centers=4, cluster_std=1.5, random_state=42)\n",
        "db = DBSCAN(eps=0.8, min_samples=7).fit(X)\n",
        "labels = db.labels_\n",
        "\n",
        "n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "n_noise = list(labels).count(-1)\n",
        "\n",
        "print(f\"Number of clusters found: {n_clusters}\")\n",
        "print(f\"Number of noise points: {n_noise}\")\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='tab10')\n",
        "plt.title(f\"DBSCAN: {n_clusters} Clusters, {n_noise} Noise Points\")\n",
        "plt.show()\n",
        "\n",
        "```\n",
        "\n",
        "### **29. t-SNE + Agglomerative Clustering (Digits)**\n",
        "\n",
        "Since Digits has 64 features, t-SNE helps visualize the high-dimensional clusters in 2D space.\n",
        "\n",
        "```python\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "digits = load_digits()\n",
        "# Reduce 64D to 2D\n",
        "X_tsne = TSNE(n_components=2, perplexity=30, random_state=42).fit_transform(digits.data)\n",
        "\n",
        "# Cluster the reduced data\n",
        "agg = AgglomerativeClustering(n_clusters=10)\n",
        "labels = agg.fit_predict(X_tsne)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=labels, cmap='nipy_spectral', s=10)\n",
        "plt.colorbar()\n",
        "plt.title(\"t-SNE Visualization of Agglomerative Clusters (Digits)\")\n",
        "plt.show()\n",
        "\n",
        "```\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "ONePO3PxxQIF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "### **30. Iris Hierarchical Clustering & Dendrogram**\n",
        "\n",
        "Hierarchical clustering creates a tree of relationships. The dendrogram visualizes the distance at which clusters merge.\n",
        "\n",
        "```python\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "from sklearn.datasets import load_iris\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "iris = load_iris()\n",
        "linked = linkage(iris.data, method='average')\n",
        "plt.figure(figsize=(10, 5))\n",
        "dendrogram(linked)\n",
        "plt.title(\"Iris Dendrogram (Average Linkage)\")\n",
        "plt.show()\n",
        "\n",
        "```\n",
        "\n",
        "### **31. Overlapping Blobs & Decision Boundaries**\n",
        "\n",
        "K-Means partitions space into Voronoi cells. Even with overlapping data, the boundaries remain linear.\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "X, _ = make_blobs(n_samples=500, centers=3, cluster_std=2.5, random_state=42)\n",
        "km = KMeans(n_clusters=3).fit(X)\n",
        "\n",
        "h = .02\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
        "Z = km.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
        "\n",
        "plt.contourf(xx, yy, Z, alpha=0.3)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=km.labels_, s=10, edgecolor='k')\n",
        "plt.title(\"K-Means Decision Boundaries (Overlapping Blobs)\")\n",
        "plt.show()\n",
        "\n",
        "```\n",
        "\n",
        "### **32. t-SNE & DBSCAN (Digits Dataset)**\n",
        "\n",
        "t-SNE reduces the 64-dimensional Digits data to 2D, allowing DBSCAN to find clusters based on local density.\n",
        "\n",
        "```python\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.datasets import load_digits\n",
        "\n",
        "digits = load_digits()\n",
        "X_tsne = TSNE(n_components=2, random_state=42).fit_transform(digits.data)\n",
        "db = DBSCAN(eps=4, min_samples=5).fit(X_tsne)\n",
        "\n",
        "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=db.labels_, cmap='tab10', s=5)\n",
        "plt.title(\"DBSCAN on t-SNE Reduced Digits\")\n",
        "plt.show()\n",
        "\n",
        "```\n",
        "\n",
        "### **33. Agglomerative Clustering (Complete Linkage)**\n",
        "\n",
        "Complete linkage merges clusters based on the maximum distance between points in each cluster.\n",
        "\n",
        "```python\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "X, _ = make_blobs(n_samples=300, centers=4, random_state=42)\n",
        "agg = AgglomerativeClustering(n_clusters=4, linkage='complete')\n",
        "plt.scatter(X[:, 0], X[:, 1], c=agg.fit_predict(X), cmap='viridis')\n",
        "plt.title(\"Complete Linkage Clustering\")\n",
        "plt.show()\n",
        "\n",
        "```\n",
        "\n",
        "### **34. Elbow Method (Breast Cancer Dataset)**\n",
        "\n",
        "This plot shows the drop in inertia (SSE) as K increases. The \"elbow\" suggests the best K.\n",
        "\n",
        "```python\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "data = load_breast_cancer()\n",
        "inertia = [KMeans(n_clusters=k).fit(data.data).inertia_ for k in range(2, 7)]\n",
        "plt.plot(range(2, 7), inertia, marker='o')\n",
        "plt.title(\"Elbow Method (Breast Cancer)\")\n",
        "plt.show()\n",
        "\n",
        "```\n",
        "\n",
        "### **35. Concentric Circles (Single Linkage)**\n",
        "\n",
        "Single linkage can follow thin paths of high density, allowing it to correctly cluster nested circles.\n",
        "\n",
        "```python\n",
        "from sklearn.datasets import make_circles\n",
        "\n",
        "X, _ = make_circles(n_samples=500, factor=0.5, noise=0.05)\n",
        "agg = AgglomerativeClustering(n_clusters=2, linkage='single')\n",
        "plt.scatter(X[:, 0], X[:, 1], c=agg.fit_predict(X))\n",
        "plt.title(\"Single Linkage on Concentric Circles\")\n",
        "plt.show()\n",
        "\n",
        "```\n",
        "\n",
        "### **36. Wine Dataset: Scaled DBSCAN**\n",
        "\n",
        "Standardization is critical for distance-based algorithms like DBSCAN.\n",
        "\n",
        "```python\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "wine = load_wine()\n",
        "X_scaled = StandardScaler().fit_transform(wine.data)\n",
        "db = DBSCAN(eps=2.5, min_samples=5).fit(X_scaled)\n",
        "n_clusters = len(set(db.labels_)) - (1 if -1 in db.labels_ else 0)\n",
        "print(f\"Number of clusters (excluding noise): {n_clusters}\")\n",
        "\n",
        "```\n",
        "\n",
        "### **37. Visualizing Cluster Centers**\n",
        "\n",
        "The `cluster_centers_` attribute provides the coordinates of the centroids calculated by K-Means.\n",
        "\n",
        "```python\n",
        "X, _ = make_blobs(n_samples=300, centers=3, random_state=42)\n",
        "km = KMeans(n_clusters=3).fit(X)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=km.labels_, alpha=0.5)\n",
        "plt.scatter(km.cluster_centers_[:, 0], km.cluster_centers_[:, 1], s=200, c='red', marker='X', label='Centroids')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "```\n",
        "\n",
        "### **38. Iris Noise Detection with DBSCAN**\n",
        "\n",
        "Points labeled as `-1` are considered outliers by the DBSCAN algorithm.\n",
        "\n",
        "```python\n",
        "iris = load_iris()\n",
        "db = DBSCAN(eps=0.5, min_samples=5).fit(iris.data)\n",
        "noise_count = list(db.labels_).count(-1)\n",
        "print(f\"Number of noise samples in Iris: {noise_count}\")\n",
        "\n",
        "```\n",
        "\n",
        "### **39. Non-Linear Moons (K-Means Failure)**\n",
        "\n",
        "K-Means assumes clusters are spherical and will fail to correctly cluster \"moon\" shapes.\n",
        "\n",
        "```python\n",
        "from sklearn.datasets import make_moons\n",
        "X, _ = make_moons(n_samples=200, noise=0.05, random_state=42)\n",
        "km = KMeans(n_clusters=2).fit_predict(X)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=km)\n",
        "plt.title(\"K-Means failure on non-linear data\")\n",
        "plt.show()\n",
        "\n",
        "```\n",
        "\n",
        "### **40. 3D PCA Visualization (Digits)**\n",
        "\n",
        "Visualizing the first three principal components provides a spatial view of how clusters separate.\n",
        "\n",
        "```python\n",
        "from sklearn.decomposition import PCA\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "digits = load_digits()\n",
        "X_pca = PCA(n_components=3).fit_transform(digits.data)\n",
        "km = KMeans(n_clusters=10).fit_predict(X_pca)\n",
        "\n",
        "fig = plt.figure(figsize=(8, 6))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2], c=km, s=5)\n",
        "plt.title(\"3D PCA Clustering (Digits)\")\n",
        "plt.show()\n",
        "\n",
        "```\n",
        "\n",
        "### **41. Silhouette Score (5 Centers)**\n",
        "\n",
        "The Silhouette Score evaluates how similar an object is to its own cluster compared to others.\n",
        "\n",
        "```python\n",
        "from sklearn.metrics import silhouette_score\n",
        "X, _ = make_blobs(n_samples=500, centers=5, random_state=42)\n",
        "km = KMeans(n_clusters=5).fit(X)\n",
        "print(f\"Silhouette Score: {silhouette_score(X, km.labels_):.3f}\")\n",
        "\n",
        "```\n",
        "\n",
        "### **42. PCA + Agglomerative (Breast Cancer)**\n",
        "\n",
        "Dimensionality reduction before clustering helps in visualizing the high-dimensional cancer data.\n",
        "\n",
        "```python\n",
        "X_pca = PCA(n_components=2).fit_transform(StandardScaler().fit_transform(data.data))\n",
        "agg = AgglomerativeClustering(n_clusters=2).fit_predict(X_pca)\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=agg, cmap='coolwarm')\n",
        "plt.title(\"Agglomerative Clustering on PCA Components\")\n",
        "plt.show()\n",
        "\n",
        "```\n",
        "\n",
        "### **43. Silhouette Plot (Iris)**\n",
        "\n",
        "A per-sample visualization of the Silhouette Coefficient.\n",
        "\n",
        "```python\n",
        "from sklearn.metrics import silhouette_samples\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "X = load_iris().data\n",
        "km = KMeans(n_clusters=3, random_state=42)\n",
        "labels = km.fit_predict(X)\n",
        "sample_values = silhouette_samples(X, labels)\n",
        "\n",
        "# Plotting each cluster's silhouette values (logic simplified)\n",
        "for i in range(3):\n",
        "    vals = sample_values[labels == i]\n",
        "    vals.sort()\n",
        "    plt.fill_betweenx(np.arange(len(vals)), 0, vals)\n",
        "plt.title(\"Silhouette Coefficient per Sample\")\n",
        "plt.show()\n",
        "\n",
        "```\n",
        "\n",
        "### **44. Wine Pairplot (Seaborn)**\n",
        "\n",
        "Visualizing cluster assignments across multiple features simultaneously.\n",
        "\n",
        "```python\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(wine.data[:, :4], columns=wine.feature_names[:4])\n",
        "df['Cluster'] = KMeans(n_clusters=3).fit_predict(StandardScaler().fit_transform(wine.data))\n",
        "sns.pairplot(df, hue='Cluster')\n",
        "plt.show()\n",
        "\n",
        "```\n",
        "\n",
        "### **45. t-SNE + Agglomerative (Digits)**\n",
        "\n",
        "Combining non-linear dimensionality reduction with hierarchical grouping.\n",
        "\n",
        "```python\n",
        "X_tsne = TSNE(n_components=2, random_state=42).fit_transform(digits.data)\n",
        "agg = AgglomerativeClustering(n_clusters=10).fit_predict(X_tsne)\n",
        "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=agg, cmap='nipy_spectral', s=5)\n",
        "plt.title(\"Agglomerative Clustering on t-SNE\")\n",
        "plt.show()\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "cUuqkGWnxuhK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### **46. Side-by-Side: K-Means vs. DBSCAN (Noisy Circles)**\n",
        "\n",
        "This exercise demonstrates why density-based clustering is superior for non-linear structures. K-Means attempts to split the circles into two halves, while DBSCAN identifies the inner and outer rings.\n",
        "\n",
        "```python\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_circles\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Generate noisy circular data\n",
        "X, _ = make_circles(n_samples=500, factor=0.5, noise=0.08, random_state=42)\n",
        "X_scaled = StandardScaler().fit_transform(X)\n",
        "\n",
        "# Apply Algorithms\n",
        "km_labels = KMeans(n_clusters=2, random_state=42).fit_predict(X_scaled)\n",
        "db_labels = DBSCAN(eps=0.3, min_samples=5).fit_predict(X_scaled)\n",
        "\n",
        "# Plot Side-by-Side\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "ax1.scatter(X_scaled[:, 0], X_scaled[:, 1], c=km_labels, cmap='viridis')\n",
        "ax1.set_title(\"K-Means Clustering (Incorrect)\")\n",
        "\n",
        "ax2.scatter(X_scaled[:, 0], X_scaled[:, 1], c=db_labels, cmap='plasma')\n",
        "ax2.set_title(\"DBSCAN Clustering (Correct)\")\n",
        "plt.show()\n",
        "\n",
        "```\n",
        "\n",
        "### **47. Agglomerative Clustering: 'Average' Linkage (Blobs)**\n",
        "\n",
        "Average linkage calculates the distance between two clusters as the average distance between every point in one cluster and every point in the other. It is generally more robust than 'single' linkage.\n",
        "\n",
        "```python\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "X, _ = make_blobs(n_samples=300, centers=3, random_state=42)\n",
        "agg = AgglomerativeClustering(n_clusters=3, linkage='average')\n",
        "labels = agg.fit_predict(X)\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='rainbow')\n",
        "plt.title(\"Agglomerative Clustering: Average Linkage\")\n",
        "plt.show()\n",
        "\n",
        "```\n",
        "\n",
        "### **48. DBSCAN: Identifying Clusters and Noise Points**\n",
        "\n",
        "In DBSCAN, any point not belonging to a cluster is assigned the label `-1`. This code counts both the distinct groups found and the outliers.\n",
        "\n",
        "```python\n",
        "X, _ = make_blobs(n_samples=400, centers=4, cluster_std=1.5, random_state=42)\n",
        "db = DBSCAN(eps=0.8, min_samples=7).fit(X)\n",
        "labels = db.labels_\n",
        "\n",
        "# Identify number of clusters (ignoring noise) and noise points\n",
        "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "n_noise_ = list(labels).count(-1)\n",
        "\n",
        "print(f\"Number of clusters: {n_clusters_}\")\n",
        "print(f\"Number of noise points: {n_noise_}\")\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='tab10')\n",
        "plt.title(f\"DBSCAN: {n_clusters_} Clusters and {n_noise_} Noise Points\")\n",
        "plt.show()\n",
        "\n",
        "```\n",
        "\n",
        "### **49. t-SNE + Agglomerative Clustering (Digits)**\n",
        "\n",
        "For complex datasets like Digits, hierarchical clustering works significantly better in a reduced 2D space where the local geometry is preserved.\n",
        "\n",
        "```python\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.datasets import load_digits\n",
        "\n",
        "digits = load_digits()\n",
        "# Reduce 64D to 2D using t-SNE\n",
        "X_tsne = TSNE(n_components=2, perplexity=30, random_state=42).fit_transform(digits.data)\n",
        "\n",
        "# Apply Agglomerative Clustering on the 2D representation\n",
        "agg = AgglomerativeClustering(n_clusters=10)\n",
        "labels = agg.fit_predict(X_tsne)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=labels, cmap='nipy_spectral', s=10)\n",
        "plt.colorbar(label='Digit Cluster')\n",
        "plt.title(\"Digits: t-SNE Reduction with Agglomerative Clustering\")\n",
        "plt.show()\n",
        "\n",
        "```\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "3Y3xf09zx5Vz"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOXT5p+DmBceQjVhhARPOGN",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}