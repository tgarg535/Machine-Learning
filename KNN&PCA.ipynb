{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tgarg535/Machine-Learning/blob/main/KNN%26PCA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Theoretical Questions**\n",
        "### **1. What is K-Nearest Neighbors (KNN) and how does it work?**\n",
        "\n",
        "KNN is a **non-parametric, lazy learning** algorithm. It does not learn a discriminative function from the training data; instead, it \"memorizes\" the dataset.\n",
        "\n",
        "* **How it works:** When a new data point is introduced, the algorithm calculates the distance between that point and all other points in the training set. It then selects the  points closest to it and assigns a label based on a majority vote (Classification) or an average (Regression).\n",
        "\n",
        "### **2. What is the difference between KNN Classification and KNN Regression?**\n",
        "\n",
        "* **KNN Classification:** The output is a class membership. The object is assigned to the class most common among its  nearest neighbors. If , the object is assigned to the class of its single nearest neighbor.\n",
        "* **KNN Regression:** The output is the property value for the object. This value is the **average** (or weighted average) of the values of its  nearest neighbors.\n",
        "\n",
        "### **3. What is the role of the distance metric in KNN?**\n",
        "\n",
        "The distance metric determines how \"closeness\" is defined.\n",
        "\n",
        "* **Euclidean Distance:** The most common (straight-line distance).\n",
        "* **Manhattan Distance:** Sum of absolute differences (city block distance).\n",
        "* **Minkowski Distance:** A generalized form of both Euclidean and Manhattan.\n",
        "Choosing the right metric is vital because KNN's accuracy depends entirely on the mathematical definition of similarity.\n",
        "\n",
        "### **4. What is the Curse of Dimensionality in KNN?**\n",
        "\n",
        "As the number of features (dimensions) increases, the volume of the space grows exponentially, making the data points **sparse**. In high-dimensional space, the distance between the nearest and farthest points becomes almost the same, causing KNN to lose its predictive power because \"closeness\" becomes meaningless.\n",
        "\n",
        "### **5. How can we choose the best value of K in KNN?**\n",
        "\n",
        "* **Small K:** Low bias but high variance (sensitive to noise/outliers).\n",
        "* **Large K:** High bias but low variance (smoother decision boundaries, but may include points from other classes).\n",
        "* **Method:** We typically use **Cross-Validation**. We plot the error rate against various  values and select the \"elbow\" point where the error stabilizes.\n",
        "\n",
        "### **6. What are KD Tree and Ball Tree in KNN?**\n",
        "\n",
        "To avoid calculating the distance to *every* point (which is ), we use spatial data structures:\n",
        "\n",
        "* **KD Tree (K-Dimensional Tree):** A binary tree that partitions space into axis-aligned boxes.\n",
        "* **Ball Tree:** Partitions data points into a series of nesting hyper-spheres (balls).\n",
        "\n",
        "### **7. When should you use KD Tree vs. Ball Tree?**\n",
        "\n",
        "* **KD Tree:** Efficient for low-dimensional data (under 20 features) but becomes inefficient as dimensionality grows.\n",
        "* **Ball Tree:** Better suited for high-dimensional data because it uses hyperspheres rather than axis-aligned partitions, handling the \"curse of dimensionality\" slightly better than KD Trees.\n",
        "\n",
        "### **8. What are the disadvantages of KNN?**\n",
        "\n",
        "* **Computationally Expensive:** Since it's a lazy learner, all computation happens during the prediction phase.\n",
        "* **Memory Intensive:** Requires storing the entire dataset.\n",
        "* **Sensitive to Scale:** Features with larger magnitudes dominate the distance.\n",
        "* **Sensitive to Outliers:** A single noisy point can change the classification of nearby points if  is small.\n",
        "\n",
        "### **9. How does feature scaling affect KNN?**\n",
        "\n",
        "Since KNN relies on distance, features must be on the same scale. If one feature ranges from 0–1 and another from 0–1000, the latter will dictate the distance. **Normalization** or **Standardization** is mandatory for KNN.\n",
        "\n",
        "---\n",
        "\n",
        "## **Principal Component Analysis (PCA)**\n",
        "\n",
        "### **10. What is PCA (Principal Component Analysis)?**\n",
        "\n",
        "PCA is an unsupervised linear dimensionality reduction technique. It transforms a large set of variables into a smaller one that still contains most of the information (variance) of the original set.\n",
        "\n",
        "### **11. How does PCA work?**\n",
        "\n",
        "1. **Standardize** the data.\n",
        "2. Compute the **Covariance Matrix** to see how variables vary from the mean with respect to each other.\n",
        "3. Calculate **Eigenvectors and Eigenvalues** of the covariance matrix.\n",
        "4. Sort Eigenvalues in descending order to identify the **Principal Components**.\n",
        "5. Project the original data onto these new axes.\n",
        "\n",
        "### **12. What is the geometric intuition behind PCA?**\n",
        "\n",
        "Geometrically, PCA looks for a new coordinate system. The **First Principal Component (PC1)** is a line that passes through the data in a way that captures the maximum possible variance (the \"longest\" direction of the data cloud). Each subsequent component is perpendicular (orthogonal) to the previous one and captures the next highest variance.\n",
        "\n",
        "### **13. What are Eigenvalues and Eigenvectors in PCA?**\n",
        "\n",
        "* **Eigenvectors:** The directions of the axes where there is the most variance (the principal components).\n",
        "* **Eigenvalues:** Scalars that determine the magnitude or amount of variance captured in that specific eigenvector direction.\n",
        "\n",
        "### **14. What is the difference between Feature Selection and Feature Extraction?**\n",
        "\n",
        "* **Feature Selection:** Keeping a subset of the original variables and discarding the rest (e.g., choosing \"Age\" and \"Income\" but dropping \"Zip Code\").\n",
        "* **Feature Extraction:** Transforming data into a new set of features that are combinations of the original variables (e.g., PCA creating \"PC1\" and \"PC2\" from 10 original variables).\n",
        "\n",
        "### **15. How do you decide the number of components to keep in PCA?**\n",
        "\n",
        "We use a **Scree Plot** or the **Cumulative Explained Variance**. Usually, we select the number of components that explain 90–95% of the total variance.\n",
        "\n",
        "### **16. Can PCA be used for classification?**\n",
        "\n",
        "PCA itself is not a classifier; it is a preprocessing step. It is often used to reduce dimensions before feeding the data into a classifier like KNN or SVM to improve speed and reduce overfitting.\n",
        "\n",
        "### **17. What are the limitations of PCA?**\n",
        "\n",
        "* **Linearity:** It assumes that the relationships between variables are linear.\n",
        "* **Information Loss:** Some variance (information) is always lost during reduction.\n",
        "* **Interpretability:** Principal components are combinations of original features, making it hard to explain what a \"component\" actually represents in the real world.\n",
        "\n",
        "### **18. How do KNN and PCA complement each other?**\n",
        "\n",
        "PCA is frequently used as a **preprocessing step for KNN**. By reducing the number of dimensions, PCA mitigates the \"Curse of Dimensionality,\" making the distance calculations in KNN more meaningful and significantly faster.\n",
        "\n",
        "### **19. How does KNN handle missing values in a dataset?**\n",
        "\n",
        "While standard KNN cannot handle missing values, we can use **KNN Imputation**. To fill a missing value for a sample, we find its  nearest neighbors (using the features that *are* present) and take the mean or mode of that feature from those neighbors.\n",
        "\n",
        "### **20. What are the key differences between PCA and Linear Discriminant Analysis (LDA)?**\n",
        "\n",
        "* **PCA:** Unsupervised. It ignores class labels and focuses on maximizing variance.\n",
        "* **LDA:** Supervised. It uses class labels and focuses on maximizing the distance between different classes while minimizing the spread within each class.\n"
      ],
      "metadata": {
        "id": "9dguvbJY-A3F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "#**Practical Questions**\n",
        "\n",
        "### **21. Train a KNN Classifier on the Iris Dataset**\n",
        "\n",
        "This is the standard entry point for classification.\n",
        "\n",
        "```python\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=3)\n",
        "knn.fit(X_train, y_train)\n",
        "print(f\"Model Accuracy: {accuracy_score(y_test, knn.predict(X_test)):.2f}\")\n",
        "\n",
        "```\n",
        "\n",
        "### **22. KNN Regressor on Synthetic Data**\n",
        "\n",
        "KNN can predict continuous values by averaging the targets of the  nearest neighbors.\n",
        "\n",
        "```python\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "X = np.sort(5 * np.random.rand(40, 1), axis=0)\n",
        "y = np.sin(X).ravel() + np.random.normal(0, 0.1, X.shape[0])\n",
        "\n",
        "knr = KNeighborsRegressor(n_neighbors=5)\n",
        "knr.fit(X, y)\n",
        "mse = mean_squared_error(y, knr.predict(X))\n",
        "print(f\"Mean Squared Error: {mse:.4f}\")\n",
        "\n",
        "```\n",
        "\n",
        "### **23. Comparing Euclidean vs. Manhattan Metrics**\n",
        "\n",
        "The distance metric changes how neighbors are selected.\n",
        "\n",
        "```python\n",
        "for metric in ['euclidean', 'manhattan']:\n",
        "    knn = KNeighborsClassifier(n_neighbors=5, metric=metric)\n",
        "    knn.fit(X_train, y_train)\n",
        "    print(f\"Accuracy with {metric}: {knn.score(X_test, y_test):.2f}\")\n",
        "\n",
        "```\n",
        "\n",
        "### **24. K-Value Decision Boundaries**\n",
        "\n",
        "Small  values create jagged boundaries (overfitting), while large  values create smoother ones (underfitting).\n",
        "\n",
        "### **25. Feature Scaling vs. Unscaled Data**\n",
        "\n",
        "Since KNN depends on distance, large-scale features dominate. Scaling is mandatory.\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Compare accuracy of scaled vs unscaled here...\n",
        "\n",
        "```\n",
        "\n",
        "### **26. PCA Explained Variance Ratio**\n",
        "\n",
        "This shows the percentage of the dataset's total variance that lies along each principal component.\n",
        "\n",
        "```python\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "pca.fit(X_train_scaled)\n",
        "print(f\"Explained Variance Ratio: {pca.explained_variance_ratio_}\")\n",
        "\n",
        "```\n",
        "\n",
        "### **27. PCA as a Preprocessing Step**\n",
        "\n",
        "Reducing dimensions can sometimes improve KNN accuracy by removing noise.\n",
        "\n",
        "```python\n",
        "# Apply PCA, then feed into KNN and compare with raw KNN results\n",
        "\n",
        "```\n",
        "\n",
        "### **28. Hyperparameter Tuning with GridSearchCV**\n",
        "\n",
        "Automate the search for the best  and distance metric.\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "params = {'n_neighbors': range(1, 20), 'weights': ['uniform', 'distance']}\n",
        "grid = GridSearchCV(KNeighborsClassifier(), params, cv=5)\n",
        "grid.fit(X_train_scaled, y_train)\n",
        "print(f\"Best K: {grid.best_params_['n_neighbors']}\")\n",
        "\n",
        "```\n",
        "\n",
        "### **29. Identifying Misclassified Samples**\n",
        "\n",
        "```python\n",
        "y_pred = knn.predict(X_test)\n",
        "misclassified = (y_test != y_pred).sum()\n",
        "print(f\"Number of misclassified samples: {misclassified}\")\n",
        "\n",
        "```\n",
        "\n",
        "### **30. Cumulative Explained Variance Plot**\n",
        "\n",
        "This helps decide how many components to keep.\n",
        "\n",
        "```python\n",
        "pca = PCA().fit(X_train_scaled)\n",
        "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Cumulative Explained Variance')\n",
        "plt.show()\n",
        "\n",
        "```\n",
        "\n",
        "### **31. Weights: Uniform vs. Distance**\n",
        "\n",
        "'Distance' weights give closer neighbors more influence on the prediction.\n",
        "\n",
        "```python\n",
        "# Compare KNeighborsClassifier(weights='uniform') vs weights='distance'\n",
        "\n",
        "```\n",
        "\n",
        "### **32. KNN Imputation for Missing Values**\n",
        "\n",
        "```python\n",
        "from sklearn.impute import KNNImputer\n",
        "import numpy as np\n",
        "\n",
        "X_missing = [[1, 2, np.nan], [3, 4, 3], [np.nan, 6, 5], [8, 8, 7]]\n",
        "imputer = KNNImputer(n_neighbors=2)\n",
        "print(imputer.fit_transform(X_missing))\n",
        "\n",
        "```\n",
        "\n",
        "### **33. PCA Data Projection (2D)**\n",
        "\n",
        "### **34. KD Tree vs. Ball Tree Performance**\n",
        "\n",
        "Both are algorithms used to speed up the search for nearest neighbors.\n",
        "\n",
        "```python\n",
        "import time\n",
        "for algo in ['kd_tree', 'ball_tree']:\n",
        "    start = time.time()\n",
        "    KNeighborsClassifier(algorithm=algo).fit(X_train, y_train).predict(X_test)\n",
        "    print(f\"{algo} time: {time.time() - start:.5f}s\")\n",
        "\n",
        "```\n",
        "\n",
        "### **35. PCA Scree Plot**\n",
        "\n",
        "A bar chart of individual variances to find the \"elbow.\"\n",
        "\n",
        "### **36. Classification Report (Wine Dataset)**\n",
        "\n",
        "```python\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "wine = load_wine()\n",
        "# Split, Scale, Train KNN...\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "```\n",
        "\n",
        "### **37. KNN Classifier with ROC-AUC**\n",
        "\n",
        "Useful for evaluating the performance of a classifier at various threshold settings.\n",
        "\n",
        "```python\n",
        "from sklearn.metrics import roc_auc_score\n",
        "# Note: For multi-class, use multi_class='ovr'\n",
        "\n",
        "```\n",
        "\n",
        "### **38. Data Reconstruction Error**\n",
        "\n",
        "Measure how much information is lost when you compress data with PCA and then decompress it.\n",
        "\n",
        "```python\n",
        "X_reduced = pca.transform(X_test_scaled)\n",
        "X_recovered = pca.inverse_transform(X_reduced)\n",
        "loss = np.mean((X_test_scaled - X_recovered) ** 2)\n",
        "print(f\"Reconstruction Error: {loss:.5f}\")\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "### **41. KNN Weights: Uniform vs. Distance**\n",
        "\n",
        "In 'uniform' weighting, all neighbors have an equal vote. In 'distance' weighting, closer neighbors have a higher influence.\n",
        "\n",
        "```python\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "for weight in ['uniform', 'distance']:\n",
        "    knn = KNeighborsClassifier(n_neighbors=5, weights=weight)\n",
        "    knn.fit(X_train, y_train)\n",
        "    print(f\"Accuracy with weights='{weight}': {knn.score(X_test, y_test):.4f}\")\n",
        "\n",
        "```\n",
        "\n",
        "### **42. KNN Regressor: Impact of K Values**\n",
        "\n",
        "As  increases, the regression line becomes smoother (lower variance, higher bias).\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "\n",
        "X = np.sort(5 * np.random.rand(40, 1), axis=0)\n",
        "y = np.sin(X).ravel()\n",
        "\n",
        "for k in [1, 5, 20]:\n",
        "    knr = KNeighborsRegressor(n_neighbors=k)\n",
        "    y_pred = knr.fit(X, y).predict(X)\n",
        "    plt.plot(X, y_pred, label=f'K={k}')\n",
        "\n",
        "plt.scatter(X, y, color='black', label='Data')\n",
        "plt.legend()\n",
        "plt.title(\"Effect of K on Regression Line\")\n",
        "plt.show()\n",
        "\n",
        "```\n",
        "\n",
        "### **43. KNN Imputation for Missing Values**\n",
        "\n",
        "This replaces `NaN` values with the mean value from the  nearest neighbors found in the remaining features.\n",
        "\n",
        "```python\n",
        "from sklearn.impute import KNNImputer\n",
        "\n",
        "X_missing = [[1, 2, np.nan], [3, 4, 3], [np.nan, 6, 5], [8, 8, 7]]\n",
        "imputer = KNNImputer(n_neighbors=2)\n",
        "X_filled = imputer.fit_transform(X_missing)\n",
        "\n",
        "print(\"Imputed Data:\\n\", X_filled)\n",
        "\n",
        "```\n",
        "\n",
        "### **44. PCA: 2D Data Projection**\n",
        "\n",
        "Projecting high-dimensional data (like the 4D Iris dataset) into 2D allows us to visualize class separation.\n",
        "\n",
        "```python\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis')\n",
        "plt.xlabel('First Principal Component')\n",
        "plt.ylabel('Second Principal Component')\n",
        "plt.title(\"PCA Projection of Iris Dataset\")\n",
        "plt.show()\n",
        "\n",
        "```\n",
        "\n",
        "### **45. KD Tree vs. Ball Tree Performance**\n",
        "\n",
        "This compares the speed of neighbor searches. Ball Trees are generally faster for high-dimensional data.\n",
        "\n",
        "```python\n",
        "import time\n",
        "for algo in ['kd_tree', 'ball_tree']:\n",
        "    knn = KNeighborsClassifier(algorithm=algo)\n",
        "    start = time.time()\n",
        "    knn.fit(X_train, y_train).predict(X_test)\n",
        "    print(f\"{algo} Execution Time: {time.time() - start:.6f} seconds\")\n",
        "\n",
        "```\n",
        "\n",
        "### **46. PCA Scree Plot**\n",
        "\n",
        "A Scree plot helps identify the \"elbow\" to determine how many principal components are necessary.\n",
        "\n",
        "```python\n",
        "pca = PCA().fit(X)\n",
        "plt.bar(range(1, len(pca.explained_variance_ratio_) + 1), pca.explained_variance_ratio_)\n",
        "plt.step(range(1, len(pca.explained_variance_ratio_) + 1), np.cumsum(pca.explained_variance_ratio_), where='mid')\n",
        "plt.ylabel('Explained Variance Ratio')\n",
        "plt.xlabel('Principal Components')\n",
        "plt.title('Scree Plot')\n",
        "plt.show()\n",
        "\n",
        "```\n",
        "\n",
        "### **47. KNN: Precision, Recall, and F1-Score**\n",
        "\n",
        "These metrics provide a deeper look at model performance than accuracy alone, especially for imbalanced data.\n",
        "\n",
        "```python\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=3).fit(X_train, y_train)\n",
        "y_pred = knn.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "```\n",
        "\n",
        "### **48. Number of Components vs. Accuracy**\n",
        "\n",
        "This analyzes how many principal components are needed to maintain high classification accuracy.\n",
        "\n",
        "```python\n",
        "results = []\n",
        "for n in range(1, 5):\n",
        "    pca = PCA(n_components=n)\n",
        "    X_train_pca = pca.fit_transform(X_train)\n",
        "    X_test_pca = pca.transform(X_test)\n",
        "    \n",
        "    knn = KNeighborsClassifier().fit(X_train_pca, y_train)\n",
        "    results.append(knn.score(X_test_pca, y_test))\n",
        "\n",
        "plt.plot(range(1, 5), results, marker='o')\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('KNN Accuracy')\n",
        "plt.title('PCA Components vs. Accuracy')\n",
        "plt.show()\n",
        "\n",
        "```\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "P7cQgIMD9zTC"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMQeO4epYsITSvC9R+exCK6",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}