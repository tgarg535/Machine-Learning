{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### **1. What is Simple Linear Regression?**  \n",
        "Simple Linear Regression is a statistical method used to model the relationship between one independent variable (X) and one dependent variable (Y) by fitting a straight line:  \n",
        "\\[\n",
        "Y = mX + c\n",
        "\\]\n",
        "\n",
        "---\n",
        "\n",
        "### **2. What are the Key Assumptions of Simple Linear Regression?**  \n",
        "- Linearity between X and Y  \n",
        "- Independence of observations  \n",
        "- Homoscedasticity (constant variance of errors)  \n",
        "- Normal distribution of residuals  \n",
        "- No multicollinearity (not applicable for single predictor)\n",
        "\n",
        "---\n",
        "\n",
        "### **3. What Does the Coefficient *m* Represent in the Equation Y = mX + c?**  \n",
        "*m* is the **slope**, which shows how much Y changes for a one-unit increase in X.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. What Does the Intercept *c* Represent in the Equation Y = mX + c?**  \n",
        "*c* is the value of Y when X = 0. It is the point where the line intersects the Y-axis.\n",
        "\n",
        "---\n",
        "\n",
        "### **5. How Do We Calculate the Slope *m* in Simple Linear Regression?**  \n",
        "\\[\n",
        "m = \\frac{n(\\sum XY) - (\\sum X)(\\sum Y)}{n(\\sum X^2) - (\\sum X)^2}\n",
        "\\]  \n",
        "Or:  \n",
        "\\[\n",
        "m = \\frac{\\text{Cov}(X, Y)}{\\text{Var}(X)}\n",
        "\\]\n",
        "\n",
        "---\n",
        "\n",
        "### **6. What is the Purpose of the Least Squares Method in Simple Linear Regression?**  \n",
        "To minimize the sum of the squared differences between actual and predicted values (residuals), resulting in the best-fitting line.\n",
        "\n",
        "---\n",
        "\n",
        "### **7. How is the Coefficient of Determination (R²) Interpreted in Simple Linear Regression?**  \n",
        "R² indicates the proportion of variance in Y that is explained by X.  \n",
        "- R² = 0 → No explanatory power  \n",
        "- R² = 1 → Perfect prediction\n",
        "\n",
        "---\n",
        "\n",
        "### **8. What is Multiple Linear Regression?**  \n",
        "A method to model the relationship between one dependent variable and two or more independent variables.  \n",
        "\\[\n",
        "Y = b_0 + b_1X_1 + b_2X_2 + \\ldots + b_nX_n\n",
        "\\]\n",
        "\n",
        "---\n",
        "\n",
        "### **9. What is the Main Difference Between Simple and Multiple Linear Regression?**  \n",
        "- Simple Linear Regression uses one predictor.  \n",
        "- Multiple Linear Regression uses two or more predictors.\n",
        "\n",
        "---\n",
        "\n",
        "### **10. What are the Key Assumptions of Multiple Linear Regression?**  \n",
        "- Linearity  \n",
        "- Independence of errors  \n",
        "- Homoscedasticity  \n",
        "- Normality of residuals  \n",
        "- No multicollinearity  \n",
        "- No autocorrelation (especially for time series data)\n",
        "\n",
        "---\n",
        "\n",
        "### **11. What is Heteroscedasticity, and How Does It Affect Multiple Linear Regression?**  \n",
        "Heteroscedasticity is when residuals have non-constant variance.  \n",
        "It affects the model by:\n",
        "- Leading to inefficient estimates  \n",
        "- Invalidating statistical tests (t-tests, F-tests)\n",
        "\n",
        "---\n",
        "\n",
        "### **12. How Can You Improve a Multiple Linear Regression Model with High Multicollinearity?**  \n",
        "- Remove or combine highly correlated variables  \n",
        "- Use PCA (Principal Component Analysis)  \n",
        "- Apply Ridge or Lasso Regression\n",
        "\n",
        "---\n",
        "\n",
        "### **13. What Are Common Techniques for Transforming Categorical Variables for Use in Regression Models?**  \n",
        "- One-hot encoding  \n",
        "- Label encoding (for ordinal variables)  \n",
        "- Dummy variables\n",
        "\n",
        "---\n",
        "\n",
        "### **14. What is the Role of Interaction Terms in Multiple Linear Regression?**  \n",
        "Interaction terms capture the combined effect of two or more variables on the dependent variable. They help model non-additive effects.\n",
        "\n",
        "---\n",
        "\n",
        "### **15. How Can the Interpretation of Intercept Differ Between Simple and Multiple Linear Regression?**  \n",
        "- In Simple Linear Regression, the intercept is the value of Y when X = 0.  \n",
        "- In Multiple Linear Regression, it's the value of Y when all X variables = 0 (which may not always make practical sense).\n",
        "\n",
        "---\n",
        "\n",
        "### **16. What is the Significance of the Slope in Regression Analysis, and How Does It Affect Predictions?**  \n",
        "The slope indicates how the dependent variable changes with a one-unit change in the independent variable. It directly affects the predictions made by the model.\n",
        "\n",
        "---\n",
        "\n",
        "### **17. What Are the Limitations of Using R² as a Sole Measure of Model Performance?**  \n",
        "- R² increases with more predictors, even if they are not useful  \n",
        "- Doesn't indicate model correctness or causality  \n",
        "- Doesn’t show overfitting or bias\n",
        "\n",
        "---\n",
        "\n",
        "### **18. How Would You Interpret a Large Standard Error for a Regression Coefficient?**  \n",
        "A large standard error suggests high uncertainty in the estimate. The coefficient may not be statistically significant.\n",
        "\n",
        "---\n",
        "\n",
        "### **19. What is Polynomial Regression?**  \n",
        "A form of regression where the relationship between the independent variable and the dependent variable is modeled as an nth-degree polynomial.\n",
        "\n",
        "---\n",
        "\n",
        "### **20. When is Polynomial Regression Used?**  \n",
        "When data shows a **non-linear** relationship that cannot be captured by a straight line.\n",
        "\n",
        "---\n",
        "\n",
        "### **21. How Does the Intercept in a Regression Model Provide Context for the Relationship Between Variables?**  \n",
        "It gives the expected value of the dependent variable when all independent variables are zero, helping to interpret the model's baseline.\n",
        "\n",
        "---\n",
        "\n",
        "### **22. How Can Heteroscedasticity Be Identified in Residual Plots, and Why is It Important to Address It?**  \n",
        "In a residual plot, heteroscedasticity appears as a funnel-shaped pattern.  \n",
        "Addressing it is important because it violates assumptions and affects the reliability of inference.\n",
        "\n",
        "---\n",
        "\n",
        "### **23. What Does It Mean If a Multiple Linear Regression Model Has a High R² But Low Adjusted R²?**  \n",
        "It indicates **overfitting**—the model has too many predictors that don’t contribute meaningfully.\n",
        "\n",
        "---\n",
        "\n",
        "### **24. Why is It Important to Scale Variables in Multiple Linear Regression?**  \n",
        "- Ensures that all variables contribute equally  \n",
        "- Necessary for regularization methods  \n",
        "- Improves numerical stability and convergence\n",
        "\n",
        "---\n",
        "\n",
        "### **25. How Does Polynomial Regression Differ from Linear Regression?**  \n",
        "- Linear regression fits a straight line  \n",
        "- Polynomial regression fits a curved line by adding powers of the independent variable(s)\n",
        "\n",
        "---\n",
        "\n",
        "### **26. What is the General Equation for Polynomial Regression?**  \n",
        "\\[\n",
        "Y = b_0 + b_1X + b_2X^2 + \\ldots + b_nX^n\n",
        "\\]\n",
        "\n",
        "---\n",
        "\n",
        "### **27. Can Polynomial Regression Be Applied to Multiple Variables?**  \n",
        "Yes, this is known as **Multivariate Polynomial Regression**.\n",
        "\n",
        "---\n",
        "\n",
        "### **28. What Are the Limitations of Polynomial Regression?**  \n",
        "- Overfitting (especially with high degrees)  \n",
        "- Poor generalization  \n",
        "- Sensitive to outliers  \n",
        "- Harder to interpret\n",
        "\n",
        "---\n",
        "\n",
        "### **29. What Methods Can Be Used to Evaluate Model Fit When Selecting the Degree of a Polynomial?**  \n",
        "- Cross-validation  \n",
        "- Adjusted R²  \n",
        "- AIC/BIC (information criteria)  \n",
        "- Residual analysis\n",
        "\n",
        "---\n",
        "\n",
        "### **30. Why is Visualization Important in Polynomial Regression?**  \n",
        "It helps:\n",
        "- Understand the model’s fit  \n",
        "- Identify overfitting or underfitting  \n",
        "- Interpret complex relationships in data\n",
        "\n",
        "---\n",
        "\n",
        "### **31. How is Polynomial Regression Implemented in Python?**\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Degree 2 polynomial regression\n",
        "model = make_pipeline(PolynomialFeatures(degree=2), LinearRegression())\n",
        "model.fit(X, y)\n",
        "y_pred = model.predict(X)\n",
        "```\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "NFfwRyNbkmrq"
      }
    }
  ]
}