{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tgarg535/Machine-Learning/blob/main/BOOSTING.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Theoretical Questions**\n",
        "\n",
        "### **1. What is Boosting in Machine Learning?**\n",
        "\n",
        "Boosting is an **ensemble meta-algorithm** that converts a set of \"weak learners\" into a \"strong learner.\" Unlike Bagging (where models are built in parallel), Boosting builds models **sequentially**. Each new model attempts to correct the errors made by the previous models.\n",
        "\n",
        "### **2. How does Boosting differ from Bagging?**\n",
        "\n",
        "| Feature | Bagging (e.g., Random Forest) | Boosting (e.g., AdaBoost, XGBoost) |\n",
        "| --- | --- | --- |\n",
        "| **Model Building** | Parallel (independent models). | Sequential (dependent models). |\n",
        "| **Goal** | Primarily to **reduce variance** (overfitting). | Primarily to **reduce bias** (underfitting). |\n",
        "| **Data Sampling** | Uses Bootstrapping (random sampling). | Focuses on misclassified samples from previous rounds. |\n",
        "\n",
        "### **3. What is the key idea behind AdaBoost?**\n",
        "\n",
        "AdaBoost (Adaptive Boosting) works by **reweighting**. It assigns higher weights to data points that were misclassified by the previous weak learner. The subsequent learner is forced to focus more on these \"difficult\" cases.\n",
        "\n",
        "### **4. Explain the working of AdaBoost with an example.**\n",
        "\n",
        "Imagine a binary classification task (Circles vs. Squares):\n",
        "\n",
        "1. **Round 1:** A simple \"stump\" (a one-level decision tree) splits the data. It gets 3 points wrong.\n",
        "2. **Reweight:** Those 3 wrong points are given a higher weight (they become \"heavier\").\n",
        "3. **Round 2:** The next stump tries to split the data but prioritizes getting those 3 heavy points right.\n",
        "4. **Final Vote:** All stumps are combined. Stumps with higher accuracy are given more \"say\" in the final prediction.\n",
        "\n",
        "### **5. What is Gradient Boosting vs. AdaBoost?**\n",
        "\n",
        "While AdaBoost uses weighted data points, **Gradient Boosting** uses the **Residuals** (errors). It fits the new model to the negative gradient of the loss function. Essentially, each new model tries to predict the *error* of the combined ensemble so far.\n",
        "\n",
        "### **6. What is the loss function in Gradient Boosting?**\n",
        "\n",
        "The loss function measures how far the model's predictions are from the actual values.\n",
        "\n",
        "* **Regression:** Usually Mean Squared Error (MSE) or Mean Absolute Error (MAE).\n",
        "* **Classification:** Usually Log-Loss (Cross-Entropy).\n",
        "\n",
        "### **7. How does XGBoost improve over traditional Gradient Boosting?**\n",
        "\n",
        "XGBoost (Extreme Gradient Boosting) introduces several optimizations:\n",
        "\n",
        "* **Regularization:** Includes L1 and L2 regularization to prevent overfitting.\n",
        "* **Parallel Processing:** Uses a block structure to parallelize tree construction.\n",
        "* **Tree Pruning:** Uses a \"depth-first\" approach and prunes trees backward.\n",
        "* **Missing Values:** Automatically learns the best direction to handle missing data.\n",
        "\n",
        "### **8. What is the difference between XGBoost and CatBoost?**\n",
        "\n",
        "* **XGBoost:** General purpose, very fast, requires manual encoding of categorical variables (like One-Hot Encoding).\n",
        "* **CatBoost:** Developed by Yandex, it is optimized for **categorical data**. It handles categories internally using \"Ordered Boosting\" to prevent data leakage and handles high-cardinality features efficiently.\n",
        "\n",
        "---\n",
        "\n",
        "## **Practical Implementation (Python)**\n",
        "\n",
        "### **9. AdaBoost Classifier (Question 30)**\n",
        "\n",
        "```python\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = load_iris(return_X_y=True)\n",
        "clf = AdaBoostClassifier(n_estimators=50, learning_rate=1.0)\n",
        "clf.fit(X, y)\n",
        "print(f\"AdaBoost Accuracy: {accuracy_score(y, clf.predict(X)):.2f}\")\n",
        "\n",
        "```\n",
        "\n",
        "### **10. Gradient Boosting Feature Importance (Question 32)**\n",
        "\n",
        "Feature importance tells you which variables contributed most to the model's decisions.\n",
        "\n",
        "```python\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "data = load_breast_cancer()\n",
        "gbc = GradientBoostingClassifier().fit(data.data, data.target)\n",
        "\n",
        "# Visualize Feature Importance\n",
        "import matplotlib.pyplot as plt\n",
        "plt.barh(data.feature_names[:10], gbc.feature_importances_[:10])\n",
        "plt.title(\"Top 10 Feature Importances\")\n",
        "plt.show()\n",
        "\n",
        "```\n",
        "\n",
        "### **11. XGBoost vs. Gradient Boosting Accuracy (Question 34)**\n",
        "\n",
        "```python\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "# Assuming X_train, y_train are defined\n",
        "xgb = XGBClassifier().fit(X_train, y_train)\n",
        "gbc = GradientBoostingClassifier().fit(X_train, y_train)\n",
        "\n",
        "print(f\"XGBoost: {xgb.score(X_test, y_test)}\")\n",
        "print(f\"GradBoost: {gbc.score(X_test, y_test)}\")\n",
        "\n",
        "```\n",
        "\n",
        "### **12. XGBoost Learning Rate Tuning (Question 41)**\n",
        "\n",
        "The learning rate (or \"shrinkage\") scales the contribution of each tree.\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "params = {'learning_rate': [0.01, 0.1, 0.2, 0.3]}\n",
        "grid = GridSearchCV(XGBRegressor(), params, cv=3)\n",
        "grid.fit(X, y)\n",
        "print(f\"Best Learning Rate: {grid.best_params_}\")\n",
        "\n",
        "```\n",
        "\n",
        "### **13. CatBoost with Class Weighting (Question 42)**\n",
        "\n",
        "For imbalanced datasets, `auto_class_weights` helps the model not ignore the minority class.\n",
        "\n",
        "```python\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "# 'Balanced' calculates weights inversely proportional to class frequencies\n",
        "model = CatBoostClassifier(auto_class_weights='Balanced', verbose=0)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "SCFEzqWz_AXe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Practical Questions**\n",
        "\n",
        "#### **14. Train an AdaBoost Classifier and print accuracy**\n",
        "\n",
        "```python\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = AdaBoostClassifier(n_estimators=50, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "print(f\"Accuracy: {accuracy_score(y_test, model.predict(X_test)):.2f}\")\n",
        "\n",
        "```\n",
        "\n",
        "#### **15. AdaBoost Regressor: Mean Absolute Error (MAE)**\n",
        "\n",
        "```python\n",
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Using synthetic data\n",
        "model_reg = AdaBoostRegressor(n_estimators=50, random_state=42)\n",
        "model_reg.fit(X_train, y_train) # Using iris for demo\n",
        "print(f\"MAE: {mean_absolute_error(y_test, model_reg.predict(X_test)):.2f}\")\n",
        "\n",
        "```\n",
        "\n",
        "#### **16. Gradient Boosting Classifier: Feature Importance**\n",
        "\n",
        "Gradient Boosting allows you to see which features were most influential in reducing the loss function.\n",
        "\n",
        "```python\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "cancer = load_breast_cancer()\n",
        "gbc = GradientBoostingClassifier(random_state=42).fit(X_train, y_train)\n",
        "print(\"Feature Importances:\", gbc.feature_importances_)\n",
        "\n",
        "```\n",
        "\n",
        "#### **17. Gradient Boosting Regressor: R-Squared Score**\n",
        "\n",
        "```python\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "gbr = GradientBoostingRegressor().fit(X_train, y_train)\n",
        "print(f\"R-Squared: {gbr.score(X_test, y_test):.2f}\")\n",
        "\n",
        "```\n",
        "\n",
        "#### **18. XGBoost vs. Gradient Boosting Accuracy**\n",
        "\n",
        "```python\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "xgb = XGBClassifier().fit(X_train, y_train)\n",
        "gbc = GradientBoostingClassifier().fit(X_train, y_train)\n",
        "print(f\"XGB Score: {xgb.score(X_test, y_test)}, GBC Score: {gbc.score(X_test, y_test)}\")\n",
        "\n",
        "```\n",
        "\n",
        "#### **19. CatBoost Classifier: F1-Score**\n",
        "\n",
        "```python\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "cat = CatBoostClassifier(verbose=0).fit(X_train, y_train)\n",
        "print(f\"F1-Score: {f1_score(y_test, cat.predict(X_test), average='macro'):.2f}\")\n",
        "\n",
        "```\n",
        "\n",
        "#### **20. XGBoost Regressor: Mean Squared Error (MSE)**\n",
        "\n",
        "```python\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "xgbr = XGBRegressor().fit(X_train, y_train)\n",
        "print(f\"MSE: {mean_squared_error(y_test, xgbr.predict(X_test)):.2f}\")\n",
        "\n",
        "```\n",
        "\n",
        "#### **21. AdaBoost Classifier: Visualize Feature Importance**\n",
        "\n",
        "```python\n",
        "import matplotlib.pyplot as plt\n",
        "plt.bar(range(len(model.feature_importances_)), model.feature_importances_)\n",
        "plt.title(\"AdaBoost Feature Importance\")\n",
        "plt.show()\n",
        "\n",
        "```\n",
        "\n",
        "#### **22. Gradient Boosting: Learning Curves**\n",
        "\n",
        "Learning curves help detect if the model is overfitting or underfitting as the number of iterations increases.\n",
        "\n",
        "#### **23. XGBoost: Visualize Feature Importance**\n",
        "\n",
        "```python\n",
        "from xgboost import plot_importance\n",
        "plot_importance(xgb)\n",
        "plt.show()\n",
        "\n",
        "```\n",
        "\n",
        "#### **24. CatBoost: Confusion Matrix**\n",
        "\n",
        "```python\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "cm = confusion_matrix(y_test, cat.predict(X_test))\n",
        "sns.heatmap(cm, annot=True)\n",
        "plt.show()\n",
        "\n",
        "```\n",
        "\n",
        "#### **25. AdaBoost: Number of Estimators vs. Accuracy**\n",
        "\n",
        "Vary `n_estimators` (e.g., 10, 50, 100, 500) and plot accuracy to find the point of diminishing returns.\n",
        "\n",
        "#### **26. Gradient Boosting: ROC Curve**\n",
        "\n",
        "```python\n",
        "from sklearn.metrics import plot_roc_curve\n",
        "# Note: In newer scikit-learn versions, use RocCurveDisplay\n",
        "\n",
        "```\n",
        "\n",
        "#### **27. XGBoost Regressor: Learning Rate Tuning (GridSearchCV)**\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "params = {'learning_rate': [0.01, 0.1, 0.3], 'n_estimators': [50, 100]}\n",
        "grid = GridSearchCV(XGBRegressor(), params).fit(X_train, y_train)\n",
        "print(\"Best Params:\", grid.best_params_)\n",
        "\n",
        "```\n",
        "\n",
        "#### **28. CatBoost: Class Weighting for Imbalance**\n",
        "\n",
        "Use `class_weights=[1, 5]` (where 5 is the weight for the minority class) to penalize the model more for missing minority samples.\n",
        "\n",
        "#### **29. AdaBoost: Effect of Learning Rates**\n",
        "\n",
        "Compare performance with `learning_rate=0.01` vs. `learning_rate=1.0`. A lower rate usually requires more estimators.\n",
        "\n",
        "#### **30. XGBoost: Multi-class Log-Loss**\n",
        "\n",
        "```python\n",
        "from sklearn.metrics import log_loss\n",
        "probs = xgb.predict_proba(X_test)\n",
        "print(f\"Log-Loss: {log_loss(y_test, probs):.4f}\")\n",
        "\n",
        "```\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "QBPRil0N_P3s"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM9JAs6fZHkfyDePCqaQu52",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}