{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tgarg535/Machine-Learning/blob/main/Ensemble_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Theoretical Questions**\n",
        "\n",
        "### **1. Can we use Bagging for regression problems?**\n",
        "\n",
        "**Yes.** While Bagging (Bootstrap Aggregating) is often discussed in classification, it is highly effective for regression. In a Bagging Regressor, several models (usually Decision Trees) are trained on different subsets of the data, and the final prediction is the **average** of the predictions from all individual models.\n",
        "\n",
        "### **2. Difference between multiple model training and single model training?**\n",
        "\n",
        "* **Single Model:** A single algorithm (like one Decision Tree) learns from the entire dataset. It is prone to **high variance** (overfitting) or **high bias** (underfitting).\n",
        "* **Multiple Model (Ensemble):** Multiple models are trained and their results combined. This approach captures different patterns in the data and typically results in a more **robust and generalized** model.\n",
        "\n",
        "### **3. Explain the concept of feature randomness in Random Forest.**\n",
        "\n",
        "In a standard Decision Tree, the algorithm looks at every available feature to find the best split. In **Random Forest**, for each split in each tree, the algorithm only considers a **random subset of features**. This ensures that the trees in the forest are de-correlated; even if one feature is a very strong predictor, other features will still be forced into the models of other trees.\n",
        "\n",
        "### **4. What is OOB (Out-of-Bag) Score?**\n",
        "\n",
        "When performing bootstrap sampling, roughly **1/3 of the data** is not used to train a particular tree. This unseen data is called the \"Out-of-Bag\" set. The **OOB Score** is a validation technique where each tree predicts the values for its own OOB samples. It provides an unbiased estimate of the model's accuracy without needing a separate validation set.\n",
        "\n",
        "### **5. How can you measure feature importance in Random Forest?**\n",
        "\n",
        "Random Forest measures importance by looking at how much the **prediction error increases** when a feature's values are permuted (shuffled) or how much the **Gini impurity/Mean Squared Error decreases** at nodes where that feature is used for splitting.\n",
        "\n",
        "### **6. Explain the working principle of a Bagging Classifier.**\n",
        "\n",
        "1. **Bootstrapping:** Create multiple random subsets of the training data with replacement.\n",
        "2. **Parallel Training:** Train a base classifier (e.g., a Decision Tree) on each subset independently.\n",
        "3. **Aggregating:** For a new data point, each tree provides a class prediction. The final output is determined by **Majority Voting**.\n",
        "\n",
        "### **7. How do you evaluate a Bagging Classifier’s performance?**\n",
        "\n",
        "Performance is evaluated using standard classification metrics: **Accuracy**, **Precision**, **Recall**, **F1-Score**, and the **OOB Score**. For unbalanced datasets, the **Area Under the ROC Curve (AUC-ROC)** is preferred.\n",
        "\n",
        "### **8. How does a Bagging Regressor work?**\n",
        "\n",
        "It follows the same process as the Bagging Classifier but instead of voting, it **averages** the numerical outputs of all base regressors. This averaging significantly reduces the variance of the final prediction.\n",
        "\n",
        "### **9. What is the main advantage of ensemble techniques?**\n",
        "\n",
        "The primary advantage is **Generalization**. Ensembles reduce the risk of relying on a single \"lucky\" or \"unlucky\" model. By combining multiple perspectives, they provide higher accuracy and are more stable against noise in the data.\n",
        "\n",
        "### **10. What is the main challenge of ensemble methods?**\n",
        "\n",
        "* **Computational Cost:** Training multiple models requires more memory and processing power.\n",
        "* **Interpretability:** It is much harder to explain \"why\" an ensemble made a decision compared to a single Decision Tree (the \"Black Box\" problem).\n",
        "\n",
        "---\n",
        "\n",
        "### **11. Key idea behind ensemble techniques?**\n",
        "\n",
        "The core philosophy is **\"The Wisdom of the Crowd.\"** A group of moderately performing, diverse models will collectively outperform any single expert model by canceling out individual errors.\n",
        "\n",
        "### **12. What is a Random Forest Classifier?**\n",
        "\n",
        "It is an ensemble of **Decision Trees** specifically. It improves upon basic Bagging by introducing **feature randomness** (selecting a random subset of features at each node), which makes the trees more diverse and the overall forest more accurate.\n",
        "\n",
        "### **13. Main types of ensemble techniques?**\n",
        "\n",
        "* **Bagging:** Parallel training (e.g., Random Forest).\n",
        "* **Boosting:** Sequential training (e.g., AdaBoost, XGBoost).\n",
        "* **Stacking:** Training a meta-model to combine the outputs of different base models.\n",
        "\n",
        "### **14. What is ensemble learning in machine learning?**\n",
        "\n",
        "Ensemble learning is the process by which multiple models, such as classifiers or regressors, are strategically generated and combined to solve a particular computational intelligence problem.\n",
        "\n",
        "### **15. When should we avoid using ensemble methods?**\n",
        "\n",
        "Avoid ensembles if:\n",
        "\n",
        "1. **Interpretability** is the highest priority (e.g., legal or medical explanations).\n",
        "2. The dataset is extremely small (can lead to overfitting).\n",
        "3. Computation resources or real-time latency limits are very strict.\n",
        "\n",
        "### **16. How does Bagging help in reducing overfitting?**\n",
        "\n",
        "Overfitting happens when a model captures noise as if it were a pattern. Since each tree in Bagging sees a different subset of data, noise in one subset likely won't appear in others. When the models are averaged, the **noise cancels out**, but the true patterns (which exist in all subsets) remain.\n",
        "\n",
        "### **17. Why is Random Forest better than a single Decision Tree?**\n",
        "\n",
        "A single tree is highly sensitive to the specific data it is trained on (High Variance). Random Forest averages many trees, which **stabilizes the variance** and prevents the model from being overly influenced by outliers or specific data points.\n",
        "\n",
        "### **18. What is the role of bootstrap sampling in Bagging?**\n",
        "\n",
        "Bootstrap sampling (sampling with replacement) creates **diversity**. It ensures that each model in the ensemble is trained on a slightly different version of the data, which is the foundation for the ensemble's ability to generalize.\n",
        "\n",
        "### **19. Real-world applications of ensemble techniques?**\n",
        "\n",
        "* **Banking:** Credit scoring and fraud detection.\n",
        "* **Healthcare:** Disease prediction based on multiple symptoms.\n",
        "* **E-commerce:** Recommendation engines (combining different user-behavior models).\n",
        "* **Finance:** Stock market trend prediction.\n",
        "\n",
        "### **20. What is the difference between Bagging and Boosting?**\n",
        "\n",
        "| Feature | Bagging | Boosting |\n",
        "| --- | --- | --- |\n",
        "| **Arrangement** | Parallel | Sequential |\n",
        "| **Focus** | Reduces Variance | Reduces Bias |\n",
        "| **Weighting** | All models have equal weight | Models are weighted by performance |\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "9IUpWjhA_0v8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "#**Practical Questions**\n",
        "\n",
        "### **21. Random Forest Hyperparameter Tuning (GridSearchCV)**\n",
        "\n",
        "Tuning parameters like `max_depth` and `n_estimators` is crucial to prevent overfitting.\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'min_samples_split': [2, 5]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(RandomForestClassifier(), param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
        "\n",
        "```\n",
        "\n",
        "### **22. Bagging Regressor with Different Estimator Counts**\n",
        "\n",
        "Increasing the number of base estimators usually stabilizes the error until it hits a plateau.\n",
        "\n",
        "```python\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "estimators = [10, 50, 100, 200]\n",
        "scores = []\n",
        "\n",
        "for n in estimators:\n",
        "    model = BaggingRegressor(n_estimators=n).fit(X_train, y_train)\n",
        "    scores.append(model.score(X_test, y_test))\n",
        "\n",
        "plt.plot(estimators, scores, marker='o')\n",
        "plt.title(\"Estimators vs R-squared Score\")\n",
        "plt.show()\n",
        "\n",
        "```\n",
        "\n",
        "### **23. Random Forest: Analyze Misclassified Samples**\n",
        "\n",
        "Checking which samples the model got wrong can reveal patterns of noise or overlapping classes.\n",
        "\n",
        "```python\n",
        "y_pred = rf.predict(X_test)\n",
        "misclassified_indices = (y_test != y_pred)\n",
        "print(f\"Misclassified samples count: {misclassified_indices.sum()}\")\n",
        "# Inspect first 5 misclassified instances\n",
        "print(X_test[misclassified_indices][:5])\n",
        "\n",
        "```\n",
        "\n",
        "### **24. Bagging Classifier vs. Single Decision Tree**\n",
        "\n",
        "This demonstrates the power of aggregation in reducing variance.\n",
        "\n",
        "```python\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "\n",
        "tree = DecisionTreeClassifier().fit(X_train, y_train)\n",
        "bag = BaggingClassifier(n_estimators=100).fit(X_train, y_train)\n",
        "\n",
        "print(f\"Single Tree Accuracy: {tree.score(X_test, y_test):.4f}\")\n",
        "print(f\"Bagging Accuracy: {bag.score(X_test, y_test):.4f}\")\n",
        "\n",
        "```\n",
        "\n",
        "### **25. Random Forest Confusion Matrix**\n",
        "\n",
        "The confusion matrix shows exactly where the model is confusing one class for another.\n",
        "\n",
        "```python\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "cm = confusion_matrix(y_test, rf.predict(X_test))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()\n",
        "\n",
        "```\n",
        "\n",
        "### **26. Stacking Classifier (Trees, SVM, Logistic Regression)**\n",
        "\n",
        "Stacking uses a \"Meta-Learner\" (usually Logistic Regression) to decide which base model's prediction to trust most.\n",
        "\n",
        "```python\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "estimators = [\n",
        "    ('dt', DecisionTreeClassifier()),\n",
        "    ('svc', SVC(probability=True))\n",
        "]\n",
        "\n",
        "stack_clf = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())\n",
        "stack_clf.fit(X_train, y_train)\n",
        "print(f\"Stacking Accuracy: {stack_clf.score(X_test, y_test):.4f}\")\n",
        "\n",
        "```\n",
        "\n",
        "### **27. Top 5 Important Features (Random Forest)**\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "importances = rf.feature_importances_\n",
        "feature_df = pd.DataFrame({'Feature': iris.feature_names, 'Importance': importances})\n",
        "print(feature_df.sort_values(by='Importance', ascending=False).head(5))\n",
        "\n",
        "```\n",
        "\n",
        "### **28. Bagging: Precision, Recall, and F1-score**\n",
        "\n",
        "```python\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_test, bag.predict(X_test)))\n",
        "\n",
        "```\n",
        "\n",
        "### **29. Random Forest: Effect of max_depth**\n",
        "\n",
        "Limiting `max_depth` is the primary way to control overfitting in Random Forests.\n",
        "\n",
        "```python\n",
        "depths = [1, 3, 5, 10, None]\n",
        "for d in depths:\n",
        "    rf_depth = RandomForestClassifier(max_depth=d).fit(X_train, y_train)\n",
        "    print(f\"Depth {d} Accuracy: {rf_depth.score(X_test, y_test):.4f}\")\n",
        "\n",
        "```\n",
        "\n",
        "### **30. Bagging Regressor: Tree vs. Neighbors**\n",
        "\n",
        "You can use non-tree models as base estimators for Bagging.\n",
        "\n",
        "```python\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "\n",
        "bag_tree = BaggingRegressor(base_estimator=DecisionTreeRegressor()).fit(X_train, y_train)\n",
        "bag_knn = BaggingRegressor(base_estimator=KNeighborsRegressor()).fit(X_train, y_train)\n",
        "\n",
        "print(f\"Bagging (Tree) MSE: {mean_squared_error(y_test, bag_tree.predict(X_test)):.2f}\")\n",
        "print(f\"Bagging (KNN) MSE: {mean_squared_error(y_test, bag_knn.predict(X_test)):.2f}\")\n",
        "\n",
        "```\n",
        "\n",
        "### **31. Random Forest: ROC-AUC Score**\n",
        "\n",
        "```python\n",
        "from sklearn.metrics import roc_auc_score\n",
        "# Using predict_proba for multi-class ROC-AUC\n",
        "probs = rf.predict_proba(X_test)\n",
        "print(f\"ROC-AUC Score: {roc_auc_score(y_test, probs, multi_class='ovr'):.4f}\")\n",
        "\n",
        "```\n",
        "\n",
        "### **32. Bagging: Cross-Validation**\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import cross_val_score\n",
        "scores = cross_val_score(BaggingClassifier(), X, y, cv=5)\n",
        "print(f\"Mean CV Accuracy: {scores.mean():.4f}\")\n",
        "\n",
        "```\n",
        "\n",
        "### **33. Random Forest: Precision-Recall Curve**\n",
        "\n",
        "Useful for imbalanced datasets to see the trade-off between precision and recall.\n",
        "\n",
        "### **34. Stacking: Random Forest + Logistic Regression**\n",
        "\n",
        "```python\n",
        "estimators = [('rf', RandomForestClassifier(n_estimators=10))]\n",
        "stack = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())\n",
        "stack.fit(X_train, y_train)\n",
        "\n",
        "```\n",
        "\n",
        "### **35. Bagging: Effect of Bootstrap Levels**\n",
        "\n",
        "Varying `max_samples` (the percentage of data each base learner sees) affects diversity.\n",
        "\n",
        "```python\n",
        "samples = [0.5, 0.7, 0.9, 1.0]\n",
        "for s in samples:\n",
        "    bag_samples = BaggingClassifier(max_samples=s).fit(X_train, y_train)\n",
        "    print(f\"Sample Fraction {s} Accuracy: {bag_samples.score(X_test, y_test):.4f}\")\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "### **36. Random Forest Classifier: Training with 5-Fold Cross-Validation**\n",
        "\n",
        "Cross-validation ensures that the Random Forest's performance is consistent across different subsets of the data, providing a more reliable estimate than a single train-test split.\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "X, y = load_iris(return_X_y=True)\n",
        "rf = RandomForestClassifier(n_estimators=100)\n",
        "scores = cross_val_score(rf, X, y, cv=5)\n",
        "\n",
        "print(f\"Cross-Validation Accuracy Scores: {scores}\")\n",
        "print(f\"Mean Accuracy: {scores.mean():.4f}\")\n",
        "\n",
        "```\n",
        "\n",
        "### **37. Bagging Classifier: Impact of `n_estimators` on Accuracy**\n",
        "\n",
        "This exercise helps identify the point of diminishing returns, where adding more trees no longer improves model performance.\n",
        "\n",
        "```python\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "\n",
        "n_range = [1, 5, 10, 20, 50, 100]\n",
        "accuracies = []\n",
        "\n",
        "for n in n_range:\n",
        "    clf = BaggingClassifier(n_estimators=n, random_state=42).fit(X_train, y_train)\n",
        "    accuracies.append(clf.score(X_test, y_test))\n",
        "\n",
        "plt.plot(n_range, accuracies, marker='o', linestyle='--')\n",
        "plt.title(\"Bagging: Impact of n_estimators\")\n",
        "plt.xlabel(\"Number of Estimators\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.show()\n",
        "\n",
        "```\n",
        "\n",
        "### **38. Random Forest: Visualizing the First Decision Tree**\n",
        "\n",
        "A Random Forest is an ensemble of trees. Visualizing an individual tree from the forest helps in understanding the decision-making logic of the base estimators.\n",
        "\n",
        "```python\n",
        "from sklearn.tree import plot_tree\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=10, max_depth=3).fit(X_train, y_train)\n",
        "plt.figure(figsize=(15, 10))\n",
        "plot_tree(rf.estimators_[0], feature_names=iris.feature_names, filled=True)\n",
        "plt.title(\"Visualization of a Single Tree in the Forest\")\n",
        "plt.show()\n",
        "\n",
        "```\n",
        "\n",
        "### **39. Bagging Regressor: Prediction Intervals**\n",
        "\n",
        "By looking at the predictions of every individual tree in a Bagging ensemble, we can estimate the uncertainty (variance) of the final prediction.\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "\n",
        "bag_reg = BaggingRegressor(n_estimators=100).fit(X_train, y_train)\n",
        "# Collect predictions from all 100 trees for a single test sample\n",
        "sample_preds = [tree.predict(X_test[0].reshape(1, -1)) for tree in bag_reg.estimators_]\n",
        "\n",
        "print(f\"Mean Prediction: {np.mean(sample_preds):.2f}\")\n",
        "print(f\"Standard Deviation (Uncertainty): {np.std(sample_preds):.2f}\")\n",
        "\n",
        "```\n",
        "\n",
        "### **40. Comparing Random Forest and Extra Trees Classifier**\n",
        "\n",
        "Extra Trees (Extremely Randomized Trees) differ from Random Forest by choosing split points randomly for each feature rather than searching for the best possible split, which can further reduce variance.\n",
        "\n",
        "```python\n",
        "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=100).fit(X_train, y_train)\n",
        "et = ExtraTreesClassifier(n_estimators=100).fit(X_train, y_train)\n",
        "\n",
        "print(f\"Random Forest Score: {rf.score(X_test, y_test):.4f}\")\n",
        "print(f\"Extra Trees Score: {et.score(X_test, y_test):.4f}\")\n",
        "\n",
        "```\n",
        "\n",
        "### **41. Random Forest: Analyzing Feature Correlation**\n",
        "\n",
        "If two features are highly correlated, Random Forest might split their \"importance\" score between them. This exercise identifies such relationships before training.\n",
        "\n",
        "```python\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(X, columns=iris.feature_names)\n",
        "corr_matrix = df.corr()\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='RdYlGn')\n",
        "plt.title(\"Feature Correlation Heatmap\")\n",
        "plt.show()\n",
        "\n",
        "```\n",
        "\n",
        "### **42. Bagging Classifier: Logistic Regression as Base Estimator**\n",
        "\n",
        "Bagging isn't restricted to trees; using Logistic Regression as a base learner can create a stable, low-variance linear ensemble.\n",
        "\n",
        "```python\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "\n",
        "bag_lr = BaggingClassifier(base_estimator=LogisticRegression(max_iter=1000), n_estimators=10)\n",
        "bag_lr.fit(X_train, y_train)\n",
        "print(f\"Bagging (Logistic Regression) Accuracy: {bag_lr.score(X_test, y_test):.4f}\")\n",
        "\n",
        "```\n",
        "\n",
        "### **43. Random Forest: Balanced Class Weights**\n",
        "\n",
        "For imbalanced datasets, setting `class_weight='balanced'` adjusts the weights of the classes inversely proportional to their frequencies in the input data.\n",
        "\n",
        "```python\n",
        "rf_balanced = RandomForestClassifier(n_estimators=100, class_weight='balanced')\n",
        "rf_balanced.fit(X_train, y_train)\n",
        "\n",
        "```\n",
        "\n",
        "### **44. Random Forest Regressor: Evaluating with R² and MAE**\n",
        "\n",
        "A robust evaluation uses multiple metrics to ensure the model isn't just accurate on average, but also has low absolute error.\n",
        "\n",
        "```python\n",
        "from sklearn.metrics import r2_score, mean_absolute_error\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "rf_reg = RandomForestRegressor(n_estimators=100).fit(X_train, y_train)\n",
        "y_pred = rf_reg.predict(X_test)\n",
        "\n",
        "print(f\"R² Score: {r2_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Mean Absolute Error: {mean_absolute_error(y_test, y_pred):.4f}\")\n",
        "\n",
        "```\n",
        "\n",
        "### **45. Stacking Regressor: Combining Random Forest and KNN**\n",
        "\n",
        "Stacking uses a final \"meta-regressor\" to learn how to best combine the numerical predictions of diverse base models.\n",
        "\n",
        "```python\n",
        "from sklearn.ensemble import StackingRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "estimators = [\n",
        "    ('rf', RandomForestRegressor(n_estimators=10)),\n",
        "    ('knn', KNeighborsRegressor(n_neighbors=5))\n",
        "]\n",
        "\n",
        "stack_reg = StackingRegressor(estimators=estimators, final_estimator=Ridge())\n",
        "stack_reg.fit(X_train, y_train)\n",
        "print(f\"Stacking Regressor R² Score: {stack_reg.score(X_test, y_test):.4f}\")\n",
        "\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1iIS_EWoACOP"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNQCDNf06QZrxCkCE9egG1P",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}