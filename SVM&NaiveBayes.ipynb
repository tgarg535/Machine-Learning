{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tgarg535/Machine-Learning/blob/main/SVM%26NaiveBayes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Theoretical Questions**\n",
        "\n",
        "### 1. What is a Support Vector Machine (SVM)?\n",
        "\n",
        "SVM is a supervised learning algorithm used for both classification and regression. Its primary goal is to find the **optimal hyperplane** in an N-dimensional space that distinctly classifies data points by maximizing the margin between classes.\n",
        "\n",
        "### 2. What is the difference between Hard Margin and Soft Margin SVM?\n",
        "\n",
        "* **Hard Margin:** Assumes the data is perfectly linearly separable. It allows zero misclassifications, which makes it very sensitive to outliers.\n",
        "* **Soft Margin:** Introduces a \"slack variable\" that allows some points to be misclassified or fall inside the margin. This makes the model more robust to noise and overlapping data.\n",
        "\n",
        "### 3. What is the mathematical intuition behind SVM?\n",
        "\n",
        "The goal is to maximize the **Margin**, which is the distance between the hyperplane and the nearest data points.\n",
        "Mathematically, we seek to maximize:\n",
        "\n",
        "\n",
        "\n",
        "subject to the constraint that all points are correctly classified:\n",
        "\n",
        "\n",
        "### 4. What is the role of Lagrange Multipliers in SVM?\n",
        "\n",
        "Lagrange Multipliers are used to transform the constrained optimization problem into an unconstrained dual problem. This allows us to solve for the weight vector  using only the dot products of the input vectors, which is the foundation for the **Kernel Trick**.\n",
        "\n",
        "### 5. What are Support Vectors in SVM?\n",
        "\n",
        "Support vectors are the data points that lie closest to the decision boundary (hyperplane). They are the \"critical\" points; if they were moved, the position of the hyperplane would change.\n",
        "\n",
        "### 6. What is a Support Vector Classifier (SVC) vs. Regressor (SVR)?\n",
        "\n",
        "* **SVC:** Predicts discrete class labels by finding a hyperplane that maximizes separation.\n",
        "* **SVR:** Predicts continuous values. Instead of a margin that stays \"clear\" of points, SVR tries to fit as many points as possible *within* a defined margin (epsilon-tube) around the line.\n",
        "\n",
        "### 7. What is the Kernel Trick in SVM?\n",
        "\n",
        "The Kernel Trick allows SVM to solve non-linear problems by mapping low-dimensional input space into a higher-dimensional feature space where a linear hyperplane can separate the data, without actually calculating the coordinates in that high-dimensional space.\n",
        "\n",
        "### 8. Compare Linear, Polynomial, and RBF Kernels\n",
        "\n",
        "| Kernel | Use Case | Complexity |\n",
        "| --- | --- | --- |\n",
        "| **Linear** | Linearly separable data; high-dimensional text data. | Simple, fast. |\n",
        "| **Polynomial** | Images or data where feature interactions matter. | More complex, slower. |\n",
        "| **RBF (Gaussian)** | Default for non-linear data; handles infinite dimensions. | Highly flexible but prone to overfitting. |\n",
        "\n",
        "### 9. What is the effect of the C parameter?\n",
        "\n",
        "* **Small C:** Large margin, allows more misclassifications (higher bias, lower variance).\n",
        "* **Large C:** Small margin, aims for zero misclassifications (lower bias, higher variance/overfitting).\n",
        "\n",
        "### 10. What is the role of the Gamma parameter in RBF?\n",
        "\n",
        "* **Small Gamma:** Gaussian kernel has a large reach; even far-away points influence the boundary (smoother boundary).\n",
        "* **Large Gamma:** Only points very close to the hyperplane influence it (wiggly, complex boundary).\n",
        "\n",
        "---\n",
        "\n",
        "## Part 2: Naïve Bayes Theory\n",
        "\n",
        "### 11. What is Naïve Bayes and why is it \"Naïve\"?\n",
        "\n",
        "It is a probabilistic classifier based on **Bayes' Theorem**. It is called \"Naïve\" because it makes the strong (and often unrealistic) assumption that all features are **independent** of each other given the class label.\n",
        "\n",
        "### 12. What is Bayes’ Theorem?\n",
        "\n",
        "It calculates the probability of an event based on prior knowledge of conditions:\n",
        "\n",
        "\n",
        "### 13. Explain Naïve Bayes Variants\n",
        "\n",
        "* **Gaussian:** Used when features follow a normal distribution (continuous data).\n",
        "* **Multinomial:** Used for discrete counts (e.g., word counts in text classification).\n",
        "* **Bernoulli:** Used for binary/boolean features (e.g., word presence vs. absence).\n",
        "\n",
        "### 14. What are the key assumptions?\n",
        "\n",
        "1. **Feature Independence:** Features do not influence each other.\n",
        "2. **Equal Importance:** Each feature contributes equally to the outcome.\n",
        "\n",
        "### 15. Advantages and Disadvantages\n",
        "\n",
        "* **Pros:** Extremely fast, works well with high-dimensional data, requires less training data.\n",
        "* **Cons:** The independence assumption rarely holds in the real world.\n",
        "\n",
        "### 16. Why is it good for Text Classification?\n",
        "\n",
        "Naïve Bayes handles high-dimensional sparse data (like word counts) efficiently. Even though words in a sentence aren't independent, the algorithm still captures the \"essence\" of the document class effectively.\n",
        "\n",
        "### 17. How does Laplace Smoothing help?\n",
        "\n",
        "If a word appears in the test set but not the training set, the probability becomes **zero**, which nullifies the entire calculation. Laplace Smoothing adds a small value (usually 1) to all counts to ensure no probability is ever exactly zero.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Q__n-QrEBHic"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### 18. Why is Naïve Bayes a good choice for text classification?\n",
        "\n",
        "Naïve Bayes is a staple for text classification (like spam detection or sentiment analysis) for several reasons:\n",
        "\n",
        "* **High Dimensionality:** Text data often has thousands of features (unique words). Naïve Bayes handles high-dimensional sparse data very efficiently.\n",
        "* **Decoupled Parameters:** It treats each word independently, meaning it only needs to estimate the probability of each word appearing in a class, rather than complex word combinations.\n",
        "* **Speed:** Since it only involves simple counting and multiplication, it is incredibly fast to train and predict compared to iterative models.\n",
        "\n",
        "### 19. Compare SVM and Naïve Bayes for classification tasks\n",
        "\n",
        "| Feature | Naïve Bayes | SVM |\n",
        "| --- | --- | --- |\n",
        "| **Model Type** | Probabilistic (Generative) | Geometric (Discriminative) |\n",
        "| **Speed** | Extremely fast | Slower on large datasets |\n",
        "| **Linearity** | Linear by nature | Linear or Non-linear (via Kernels) |\n",
        "| **Data Size** | Works well with small/medium data | Requires more tuning but scales well |\n",
        "| **Assumptions** | Assumes feature independence | No strong statistical assumptions |\n",
        "\n",
        "### 20. How does Laplace Smoothing help in Naïve Bayes?\n",
        "\n",
        "In text data, if a word appears in the test set but was never seen in the training set for a specific class, the probability  becomes . Since Naïve Bayes multiplies these probabilities, one zero will make the entire class probability zero.\n",
        "**Laplace Smoothing** adds a small positive value (usually ) to the numerator and adjusts the denominator to ensure no probability is ever exactly zero.\n",
        "\n",
        "---\n",
        "\n",
        "#**Practical Questions**\n",
        "\n",
        "### SVM Hyperparameter Tuning (GridSearchCV)\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2)\n",
        "\n",
        "# Defining parameter range\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'gamma': [1, 0.1, 0.01, 0.001],\n",
        "    'kernel': ['rbf', 'poly', 'linear']\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(SVC(), param_grid, refit=True, verbose=1)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print(f\"Best Parameters: {grid.best_params_}\")\n",
        "print(f\"Best Score: {grid.best_score_}\")\n",
        "\n",
        "```\n",
        "\n",
        "### Comparing SVM Kernels (Breast Cancer Dataset)\n",
        "\n",
        "```python\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "kernels = ['linear', 'poly', 'rbf']\n",
        "for k in kernels:\n",
        "    model = SVC(kernel=k, degree=3) # degree used for poly\n",
        "    model.fit(X_train, y_train)\n",
        "    pred = model.predict(X_test)\n",
        "    print(f\"Accuracy with {k} kernel: {accuracy_score(y_test, pred):.4f}\")\n",
        "\n",
        "```\n",
        "\n",
        "### SVM Regressor (SVR) with MAE Evaluation\n",
        "\n",
        "```python\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "\n",
        "housing = fetch_california_housing()\n",
        "X_h, y_h = housing.data[:1000], housing.target[:1000] # Subset for speed\n",
        "\n",
        "svr = SVR(kernel='rbf')\n",
        "svr.fit(X_h, y_h)\n",
        "y_pred = svr.predict(X_h)\n",
        "\n",
        "mae = mean_absolute_error(y_h, y_pred)\n",
        "print(f\"SVR Mean Absolute Error: {mae:.4f}\")\n",
        "\n",
        "```\n",
        "\n",
        "### Visualizing Confusion Matrix and Precision-Recall Curve\n",
        "\n",
        "```python\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, PrecisionRecallDisplay\n",
        "\n",
        "# Confusion Matrix\n",
        "y_pred = grid.predict(X_test)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('SVM Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Precision-Recall Curve\n",
        "PrecisionRecallDisplay.from_estimator(grid, X_test, y_test)\n",
        "plt.title(\"SVM Precision-Recall Curve\")\n",
        "plt.show()\n",
        "\n",
        "```\n",
        "\n",
        "### Handling Imbalanced Data with Class Weighting\n",
        "\n",
        "```python\n",
        "# 'balanced' mode automatically adjusts weights inversely proportional to class frequencies\n",
        "model_weighted = SVC(kernel='linear', class_weight='balanced')\n",
        "model_weighted.fit(X_train, y_train)\n",
        "print(f\"Weighted SVM Accuracy: {model_weighted.score(X_test, y_test):.4f}\")\n",
        "\n",
        "```\n",
        "\n",
        "### Recursive Feature Elimination (RFE) for SVM\n",
        "\n",
        "```python\n",
        "from sklearn.feature_selection import RFE\n",
        "\n",
        "svc_linear = SVC(kernel=\"linear\")\n",
        "selector = RFE(estimator=svc_linear, n_features_to_select=5, step=1)\n",
        "selector = selector.fit(X_train, y_train)\n",
        "\n",
        "print(f\"Selected Features: {selector.support_}\")\n",
        "print(f\"Feature Ranking: {selector.ranking_}\")\n",
        "\n",
        "```\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "9u92wdnABm55"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMeEHnDcDkKqIxP7fuqBCoT",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}